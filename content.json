{"pages":[{"title":"","text":"手写数字识别加载数据torchvision datasets 提供了 minist 数据集下载 1234567891011121314train_data = dsets.MNIST( root='./mnist/', train=True, transform=transforms.ToTensor(), download=True,)train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)test_data = dsets.MNIST(root='./mnist/', train=False, transform=transforms.ToTensor())text_x1 = test_data.data[:2000]text_x2 = test_data.data[:2000]test_x = torch.cat((text_x1, text_x2.permute(0, 2, 1)), dim=1) / 255.test_y = test_data.targets[:2000] 构建网络使用双向多层 LSTM 对图像进行处理，串联第 n 行，第 n 列的像素作为第 n 个时间步的输入。 12345678910111213141516171819202122232425262728class RNN(nn.Module): def __init__(self, input_size, hidden_size, n_class, num_layers, dropout, bidirectional): super(RNN, self).__init__() self.rnn = nn.LSTM( input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional, ) L_size = hidden_size * 2 if bidirectional else hidden_size self.out = nn.Linear(L_size, n_class) def forward(self, x): if not hasattr(self, '_flattened'): self.rnn.flatten_parameters() setattr(self, '_flattened', True) # x shape (batch, time_step, input_size) # r_out shape (batch, time_step, output_size) # h_n shape (n_layers, batch, hidden_size) # h_c shape (n_layers, batch, hidden_size) r_out, (h_n, c_n) = self.rnn(x, None) out = self.out(r_out.mean(1)) return out 训练过程损失函数采用的是 CrossEntropyLoss，优化器采用的 Adam。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455model = RNN( input_size=INPUT_SIZE, hidden_size=hidden, n_class=N_CLASS, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)loss_func = nn.CrossEntropyLoss()model = nn.DataParallel(model, device_ids=[0, 1, 2, 3, 4])model.cuda()test_x = test_x.cuda()test_y = test_y.cuda()x = []train_acc_set = []loss_set = []test_acc_set = []step = 0for epoch in range(epochs): for b_x, b_y in train_loader: input1 = b_x.view(-1, 28, 28) input2 = b_x.permute(0, 1, 3, 2).view(-1, 28, 28) input = torch.cat((input1, input2), dim=1) input = input.cuda() b_y = b_y.cuda() output = model(input) pred_y = torch.max(output, 1)[1] loss = loss_func(output, b_y) optimizer.zero_grad() loss.backward() optimizer.step() if step % 50 == 0: x.append(step) train_acc = float((pred_y == b_y).type(torch.int).sum()) / float(len(b_y)) train_acc_set.append(train_acc) loss_set.append(loss.item()) test_output = model(test_x) pred_y = torch.max(test_output, 1)[1] test_acc = float((pred_y == test_y).type(torch.int).sum()) / float(len(test_y)) test_acc_set.append(test_acc) print('Epoch: ', epoch, 'step: ', step, '| train loss: %.4f' % loss.item(), '| train accuracy: %.2f' % train_acc, '| test accuracy: %.2f' % test_acc) step += 1 实验分析经过 10 个 epochs，最终的结果稳定在 0.97，这表明 LSTM 也有提取并记忆图像特征的能力。 猫狗分类加载数据首先对原始数据进行处理，在原始数据中 cat 和 dog 的图片都存放在一个文件夹内，首先将 cat 和 dog 的图片存放在不同的文件夹 12345mkdir catmkdir dogmv cat.*.jpg ./catmv dog.*.jpg ./dog 再利用 ImageFolder 函数加载数据 12345678910111213normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])transform = transforms.Compose([ transforms.Resize((227, 227)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize,])train_dataset = ImageFolder('/home/zx19/projects/cat_dog/dataset/train/',transform=transform)trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)validation_dataset = ImageFolder('/home/zx19/projects/cat_dog/dataset/validation/',transform=transform)validationloader = torch.utils.data.DataLoader(validation_dataset, batch_size=500, shuffle=True) 构建网络采用的是 AlexNet 构建的网络 1234567891011121314151617181920212223242526272829303132333435363738394041424344class AlexNet(nn.Module): def __init__(self, num_classes=1000): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) self.classifier = nn.Sequential( nn.Dropout(), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.classifier(x) return xdef alexnet(pretrained=False, progress=True, **kwargs): model = AlexNet(**kwargs) if pretrained: state_dict = torch.hub.load_state_dict_from_url('https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth', progress=progress) model.load_state_dict(state_dict) return model 在 alexnet 函数的参数 pretrained 为 True 时，将加载 ImageNet 的预训练模型 初始化参数1234567891011121314151617def weights_init(m): classname = m.__class__.__name__ if classname.find('Conv2d') != -1: m.weight.data.normal_(0.0, 0.02) elif classname.find('BatchNorm') != -1: m.weight.data.normal_(1.0, 0.02) m.bias.data.fill_(0) elif classname.find('Linear') != -1: nn.init.xavier_uniform_(m.weight) m.bias.data.fill_(0)model = alexnet(pretrained=False)model.classifier[6] = nn.Linear(4096, 2)alexNet = nn.DataParallel(model, device_ids=[0, 1, 2, 3, 4])alexNet = alexNet.cuda()alexNet.apply(weights_init) 训练过程损失函数采用的是 CrossEntropyLoss，优化器使用的是 SGD 123456789101112131415161718192021222324252627282930313233343536373839404142434445criterion = nn.CrossEntropyLoss()optimizer = optim.SGD(alexNet.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)epochs = 10train_losses, validation_losses = [], []loss_step = 0for e in range(epochs): for step, (images,labels) in enumerate(trainloader): images = images.cuda() labels = labels.cuda() optimizer.zero_grad() outputs = alexNet(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() loss_step += loss.item() if (step+1)%10==0: with torch.no_grad(): for images, labels in validationloader: images = images.cuda() labels = labels.cuda() outputs = alexNet(images) validation_loss = criterion(outputs, labels) equals = outputs.max(1).indices == labels accuracy = torch.mean(equals.type(torch.float)) break train_losses.append(loss.item()) validation_losses.append(validation_loss) print(&quot;Epoch: {}/{}&quot;.format(e+1, epochs), &quot; | Training Loss: {:.3f}&quot;.format(loss.item()), &quot; | Test Loss: {:.3f}&quot;.format(validation_loss.item()), &quot; | Test Accuracy: {:.3f}&quot;.format(accuracy)) 实验分析 在经过 10 个 epochs 训练后，准确率到达了 85%。期间 train loss 和 validation loss 都在持续下降，这说明增加 epochs 还有提高准确率的空间。 自动写诗加载数据1234567datas = np.load(\"/home/zx19/projects/poem/tang.npz\", allow_pickle=True)data = datas['data']index2word = datas['ix2word'].item()word2index = datas['word2ix'].item()data = torch.from_numpy(data)train_loader = DataLoader(data, batch_size=128, shuffle=True, num_workers=2) 构建网络使用了多层单向 LSTM 网络进行训练 1234567891011121314151617181920212223242526272829class Poem(nn.Module): def __init__(self, vocab_size, embedding_dim, hidden_dim): super(Poem, self).__init__() self.num_layers = 2 self.hidden_dim = hidden_dim self.embeddings = nn.Embedding(vocab_size, embedding_dim) #vocab_size:就是ix2word这个字典的长度。 self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=self.num_layers, batch_first=True,dropout=0, bidirectional=False) self.fc1 = nn.Linear(self.hidden_dim,2048) self.fc2 = nn.Linear(2048,4096) self.fc3 = nn.Linear(4096,vocab_size) def forward(self, input, hidden=None): if not hasattr(self, '_flattened'): self.lstm.flatten_parameters() setattr(self, '_flattened', True) embeds = self.embeddings(input) # [batch, seq_len] =&gt; [batch, seq_len, embed_dim] batch_size, seq_len = input.size() if hidden is None: h_0 = input.data.new(self.num_layers*1, batch_size, self.hidden_dim).fill_(0).float() c_0 = input.data.new(self.num_layers*1, batch_size, self.hidden_dim).fill_(0).float() else: h_0, c_0 = hidden output, hidden = self.lstm(embeds, (h_0, c_0))#hidden 是h,和c 这两个隐状态 output = torch.tanh(self.fc1(output)) output = torch.tanh(self.fc2(output)) output = self.fc3(output) output = output.reshape(batch_size * seq_len, -1) return output,hidden 训练过程损失函数使用的是 CrossEntropyLoss，优化器采用的是 Adam 12345678910111213141516171819202122232425262728293031323334epochs = 5model = Poem(len(word2index), embedding_dim=100, hidden_dim=1024)model = model.to(device)optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)criterion = nn.CrossEntropyLoss()model.train()for epoch in range(epochs): train_loss = 0 train_acc = 0 i = 0 for step, data in enumerate(train_loader): i += 1 inputs, labels = data[:, :-1].to(device).to(torch.long), data[:, 1:].to(device).to(torch.long) labels = labels.view(-1) optimizer.zero_grad() outputs,hidden = model(inputs) loss = criterion(outputs,labels) loss.backward() optimizer.step() _,pred = outputs.topk(1) pred = pred.view(-1) acc = (pred==labels).to(torch.float).mean() train_acc+=acc train_loss += loss.item() if (step+1)%100==0: print('Epoch: {}/{} | Loss: {} | Acc: {}'.format(epoch+1, epochs, train_loss/i, train_acc/i)) i = 0 train_loss = 0 train_acc = 0 print('Epoch: {}/{} | Loss: {} | Acc: {}'.format(epoch+1, epochs, train_loss/i, train_acc/i)) 预测部分在生成下一个预测部分的使用，采样函数使用了 softmax temperature，并不是直接去概率最大的值，temperature 参数控制了随机性，是根据每个词的概率进行采样。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def generate(model, start_words, ix2word, word2ix): results = list(start_words) start_words_len = len(start_words) # 第一个词语是&lt;START&gt; input = t.Tensor([word2ix['&lt;START&gt;']]).view(1, 1).long() hidden = None model.eval() with torch.no_grad(): for i in range(Config.max_gen_len): output, hidden = model(input, hidden) # 如果在给定的句首中，input为句首中的下一个字 if i &lt; start_words_len: w = results[i] input = input.data.new([word2ix[w]]).view(1, 1) # 否则将output作为下一个input进行 else: top_index = output.data[0].topk(1)[1][0].item() w = ix2word[top_index] results.append(w) input = input.data.new([top_index]).view(1, 1) if w == '&lt;EOP&gt;': del results[-1] break return resultsdef sample(output, temperature): output = F.softmax(output.to(torch.float64) / temperature) # output 默认是 float 32，但是 np.random.multinomial 在处理时会将数据强制转换到 float 64，会出现精度上的损失 output = output.to('cpu').detach().numpy() return np.random.multinomial(1, output[0], 1) start_words = '雪'results = list(start_words)start_words_len = len(start_words)# 第一个词语是&lt;START&gt;input = torch.Tensor([word2index['&lt;START&gt;']]).view(1, 1).long().cuda()hidden = Nonemodel.eval()with torch.no_grad(): for i in range(125): output, hidden = model(input, hidden) # 如果在给定的句首中，input为句首中的下一个字 if i &lt; start_words_len: w = results[i] input = input.data.new([word2index[w]]).view(1, 1) # 否则将output作为下一个input进行 else: top_index = sample(output, 0.5) top_index = np.where(top_index[0]==1)[0].item() w = index2word[top_index] results.append(w) input = input.data.new([top_index]).view(1, 1) if w == '&lt;EOP&gt;': del results[-1] breakprint(''.join(results)) 实验分析12Input: 月Output: 月豺有性，近问东所。一声留夜，日起相关。 由于采用了 softmax temperature 采样下一个预测词，并没有出现生成和训练集里面的诗歌一模一样的情况。 电影评论情感分类加载数据导入训练数据，构建 MovieDataset，继承于 Dataset 123456789101112131415161718192021222324252627282930class MovieDataset(Dataset): def __init__(self, path, word_vecs, word2id): self.df = pd.read_csv(path, sep='\\t', header=None) self.labels = list(self.df[0]) self.sentences = list(self.df[1]) self.word_vecs = word_vecs self.word2id = word2id def __len__(self): return len(self.df) def __getitem__(self, index): label = self.labels[index] sentence = self.sentences[index] words = sentence.split(' ')[:75] sentence_vec = [] sentence_vec = torch.zeros(1, 75, 50) for i, word in enumerate(words): sentence_vec[0][i] = torch.tensor(self.word_vecs[self.word2id[word]]) return sentence_vec, labeltrain_dataset = MovieDataset('./dataset/train.txt', word_vecs, word2id)train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)validation_dataset = MovieDataset('./dataset/validation.txt', word_vecs, word2id)validation_dataloader = DataLoader(train_dataset, batch_size=len(validation_dataset))for validation_datas, validation_labels in validation_dataloader: validation_datas = validation_datas.cuda() validation_labels = validation_labels.cuda() 加载词向量以及构建 word2index 字典12345678910111213141516171819202122232425262728293031323334353637383940def build_word2id(path): \"\"\" :param file: word2id 保存地址 :return: None \"\"\" word2id = {'_PAD_': 0} for _path in path: with open(_path, encoding='utf-8') as f: for line in f.readlines(): sp = line.strip().split() for word in sp[1:]: if word not in word2id.keys(): word2id[word] = len(word2id) return word2iddef build_word2vec(fname, word2id, save_to_path=None): \"\"\" :param fname: 预训练的word2vec. :param word2id: 语料文本中包含的词汇集. :param save_to_path: 保存训练语料库中的词组对应的word2vec到本地 :return: 语料文本中词汇集对应的word2vec向量{id: word2vec}. \"\"\" n_words = max(word2id.values()) + 1 model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True) word_vecs = np.array(np.random.uniform(-1., 1., [n_words, model.vector_size])) for word in word2id.keys(): try: word_vecs[word2id[word]] = model[word] except KeyError: pass if save_to_path: with open(save_to_path, 'w', encoding='utf-8') as f: for vec in word_vecs: vec = [str(w) for w in vec] f.write(' '.join(vec)) f.write('\\n') return word_vecsword2id = build_word2id(['./dataset/train.txt', './dataset/validation.txt'])word_vecs = build_word2vec('./dataset/wiki_word2vec_50.bin', word2id) 网络模型12345678910111213141516171819202122232425262728293031class TextCNN(nn.Module): def __init__(self, word_embedding_dimension, sentence_max_size, label_num): super(TextCNN, self).__init__() self.label_num = label_num self.conv3 = nn.Conv2d(1, 1, (3, word_embedding_dimension)) self.conv4 = nn.Conv2d(1, 1, (4, word_embedding_dimension)) self.conv5 = nn.Conv2d(1, 1, (5, word_embedding_dimension)) self.Max3_pool = nn.MaxPool2d((sentence_max_size-3+1, 1)) self.Max4_pool = nn.MaxPool2d((sentence_max_size-4+1, 1)) self.Max5_pool = nn.MaxPool2d((sentence_max_size-5+1, 1)) self.linear1 = nn.Linear(3, label_num) def forward(self, x): batch = x.shape[0] # Convolution x1 = F.relu(self.conv3(x)) x2 = F.relu(self.conv4(x)) x3 = F.relu(self.conv5(x)) # Pooling x1 = self.Max3_pool(x1) x2 = self.Max4_pool(x2) x3 = self.Max5_pool(x3) x = torch.cat((x1, x2, x3), -1) x = x.view(batch, 1, -1) x = self.linear1(x) x = x.view(-1, self.label_num) return x 训练过程1234567891011121314151617181920212223242526272829303132333435363738394041424344batch_size=32epoch=10gpu=0out_channel=2label_num=2embed_dim = 50sentence_len = 75model = TextCNN(embed_dim, sentence_len, label_num)model = model.cuda()# model.apply(weights_init)criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)count = 0loss_sum = 0acc_sum = 0for epoch in range(epoch): for datas, labels in train_dataloader: datas = datas.cuda() labels = labels.cuda() outs = model(datas) loss = criterion(outs, labels) optimizer.zero_grad() loss.backward() optimizer.step() loss_sum += loss.item() count += 1 if count % 100 == 0: with torch.no_grad(): validation_outs = model(validation_datas) validation_loss = criterion(validation_outs, validation_labels) validation_acc = (validation_outs.topk(1)[1].view(-1)==validation_labels).to(torch.float).mean() print(\"Epoch: {} | train_loss: {:.3}, v_loss: {:.3}, v_acc: {:.3}\".format(epoch+1, loss_sum/100, validation_loss, validation_acc)) loss_sum = 0 count = 0 acc_sum = 0 实验分析12345Epoch: 8 | train_loss: 0.467, v_loss: 0.449, v_acc: 0.783Epoch: 8 | train_loss: 0.476, v_loss: 0.481, v_acc: 0.764Epoch: 8 | train_loss: 0.479, v_loss: 0.433, v_acc: 0.797Epoch: 8 | train_loss: 0.457, v_loss: 0.451, v_acc: 0.781Epoch: 8 | train_loss: 0.457, v_loss: 0.398, v_acc: 0.818 在第 8 个 epochs 的时候，在验证机上的准确率达到了 80%。textcnn 虽然说训练速度快，但是效果相比于 LSTM 较弱。","link":"/README.html"},{"title":"","text":"404 您访问的资源不存在，正在前往网站首页... setTimeout(() => { window.location.href = \"https://blog.adream.win/\"; }, 1000);","link":"/404.html"},{"title":"About me","text":"独立人格 自由思想 转发投币点赞了解一下？☞ ☞ ☞ Bilibili /* scale 用来实现宽高等比例 */ .scale { width: 100%; padding-bottom: 56.25%; height: 0; position: relative; } /* item 用来放置全部的子元素 */ .item { width: 100%; height: 100%; position: absolute; } 中国科学院大学 人工智能学院 硕士 在读 自诩是一个会技术的产品。 那些疯狂到以为自己能够改变世界的人，才能真正改变世界。我一定会倒在让这个世界更加美好的路上。 实习经历 计算所泛在研究中心 BDA 投资有限公司技术部 北京长亭科技安全策略部","link":"/about/index.html"}],"posts":[{"title":"城市异常事件检测","text":"","link":"/paper/%E5%9F%8E%E5%B8%82%E5%BC%82%E5%B8%B8%E4%BA%8B%E4%BB%B6%E6%A3%80%E6%B5%8B/"},{"title":"UCAS Affair Scripts","text":"项目地址 开发/收集 中国科学院大学 各种事务的自动化脚本 已有脚本： 选课 监听成绩 期待贡献，添加更多有趣的脚本，kill TodoList item Warning: 由于 选课系统 只能单点登录，如果同时运行两个脚本，或者在运行脚本的时候登陆网页端，他们之前会争抢会话。 脚本默认在会话失效后再次自动获取，所以在刷课和监听成绩的过程中，如果您需要浏览选课系统的网页端，需要先暂停脚本。 script12kill -STOP $pid # suspendkill -CONT $pid # resume UsageInitializationPull project: script1git clone https://github.com/zhangxu3486432/UCAS-Affair-Scripts.git Enter the working directory: script1cd UCAS-Affair-Scripts Install dependency: script1pip3 install -r deploy/requirements.txt Configuration Set in the settings.py Set http://sep.ucas.ac.cn/ UserName and PassWord 12USERNAME = ''PASSWORD = '' Login In the off-campus network login need to fill in the Verification Code. First login to save cookies in sep.cookie, so that you can facilitate deployment in the server. script1python3 login.py The Verification Code picture is saved in verification.png, open the picture identification key and fill it in terminal. As you can see, this Verification Code is 7351. Fill the 7351 in terminal and press enter. RUN take-courses Set the courses which you want to take Examples: 1COURSES = ['自然语言处理', '机器人智能控制', '积极心理学'] Fill in the college to which the course you choose belongs Examples: 1COLLEGES = ['人工智能学院', '心理学系'] Run script1python3 take_courses.py Run monitor_grades Set email info if you want to monitor your grads, you need to set it 1234SEND_EMAIL = ''SEND_EMAIL_PWD = ''RECEIVE_EMAIL = '' Run script1python3 monitor_grades.py TodoList Test and adapt to more python versions Shuttle Bus Reservation Build Docker Image Automatic Identification Verification Code","link":"/software/UCAS-Affair-Scripts/"},{"title":"Docker 设置时区","text":"简介Docker 默认时区是协调世界时，又称世界标准时间，简称 UTC 中国大陆、香港、澳门、台湾、蒙古国、新加坡、马来西亚、菲律宾、澳洲西部的时间与 UTC 的时差均为 +8，也就是 UTC+8。 中国标准时间又称为 CST(China Standard Time) 查看时区可以使用 date 命令查看时区信息 1234567➜ date# UTC 时间Sat Feb 21 16:15:16 UTC 2020➜ date# CST 时间Sat Feb 22 00:15:16 CST 2020 更改时区根据应用方式的不同修改 Docker 时区可分为三种方式 docker-compose12environment: TZ: Asia/Shanghai Dockerfile1234567891011# 需要安装 tzdata 依赖RUN apk add --no-cache tzdataENV TZ Asia/Shanghai# or# 若容器内不存在 /usr/share/zoneinfo，则需要创建这个目录RUN apk add --no-cache tzdata \\ &amp;&amp; ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ &amp;&amp; echo \"Asia/Shanghai\" &gt; /etc/timezoneENV TZ Asia/Shanghai docker run12# 容器的时区和主机的时区相同docker run -v /etc/timezone:/etc/timezone -v /etc/localtime:/etc/localtime -it [image name] sh","link":"/tutorial/docker-timezone/"},{"title":"那些年我写的现代诗","text":"记录和整理我写的现代诗 流年体警告！！！ 慎入！！！ 章一篇一 恨不能与你早相遇那样便可早相知 正好赶上三月的樱花风暖云疏 天气晴好 可终究我还是错过了那个大雪纷飞的季节我还在寻找机会与你接近却已经有人为你嘘寒问暖 篇二 崇法寺塔 高三，转移去了老校区，教室窗户外面就是两千多年前的古塔 我见证了繁华我经历过战火也许是这些原因我被永久的保护了起来这就是我的宿命像墓碑一样的伫立着 风吹日晒他们为我换上新的琉璃岁月流逝他们又为我支起了铁架 可笑的世人啊你们不知自从那持灯的僧人走后我体内剩下的只是千百座没有体温的佛像 篇三 暗恋一个人，即便不能告白，也别藏得太深吧 我把所有的爱慕埋藏在难以触及的心底好让在遇见你的时候不漏声色 我把所有的思念放置在无人翻阅的诗集好让在没有你的日子笑靥如初 后来把自己也藏了起来藏在拥挤的人群藏入忙碌的日子 篇四想你不需要理由也不分季候 想你不是从别离开始在第一次遇见你之后失眠了一整个夜一整夜的风和雨风里雨里都是你 篇五 缘由是如何清澈的月光才能刺痛温柔的心是如何呜咽地笛声才能触动眼中的泪 今夜月 清如水此间笛 声如斯 可千万别别把这误以为我悲伤的缘由 章二 求打醒章节 篇一 快，来个人，打醒他 假如上天有意为你我安排一段姻缘我便不再担心不再着急只等你来爱我 然后执子之手到天荒地老 篇二 画师 接着打，往死里打 只希望我的世界是一片空白的画布任我去着色添笔 不需要昂贵的画布也不渴求绚丽的色彩只要一杆画笔我可以在篱墙下在田间阡陌在世界上的任何一个容得下你我的角落把你画在我的怀中把我画进你的心里 篇三 不要轻易承诺吧 如果你还不懂这些苦涩的诗句那就再等上些时日 等花开花落四季轮回 如果你还不懂我有多爱你那就再等上些时日 等海枯石烂誓言未改 章三篇一 天各一方 海南的气候让我难以忍受，希望你北方快乐 我南下你北上 南下有骄阳北方应有花香 篇二 a: 你喜欢我吗，b: 你能别问了吗 喜欢 不去问夏夜有多短不去问冬夜有多长春风还会如期拂面秋雨还会如约连绵 你也不必反复追问我会待你眉眼如初 篇三好多天没能想起你这南方九十度的阳光吞噬了所有的喜怒哀乐只剩下唯有的动作从一个眉梢到另一个眉梢不经意的重复就像以前不经意的想起你 我喜欢北方的冬天那可以让我想起温暖想起温暖就会想起你我喜欢那萧萧落木那可以让我感到孤独感到孤独也会想起你 我还喜欢北方的你 篇四夜无言只将天地笼罩 我是一颗北极星悄悄来到你的夜空怕惊醒你的美梦 你是否也会深夜凭栏斗转星移我还在那里 篇五 导演你说什么就是什么天晴了，又雨了在一起便在一起说别离就是别离 后来你让我哭我也比谁哭的都卖力 篇六 雨夹雪 到家的时候下了一场雨夹雪。希望两情相悦的人都能克服困难，终成眷属吧 你是我的前生我是你的来世 今夜天空飘落的是一场雨夹雪 章四篇一 诗人竟厌恶了这安逸的生活想要背起行囊做一个诗人 流浪 流浪去千里外的渔场从港口出发借北吹的风要两天航程父亲就在那里风吹雨打半辈子 我是来感受海的孤独风的刺骨一会就走 篇二 很小的时候，贪玩，每次周日，就开着钨丝灯，趴在椅子上赶作业 关于那段岁月的一切都值得珍藏 在那个年代里有灯，昏黄的要把傍晚的时间赶回来我趴在椅子上头也不抬 篇三 小学的时候，老师经常教育我们要珍惜时间，例子就是时钟走过就转不回去… 再也回不去了那年夏天月季盛开了一个季节三叶草换成了石板路蜜蜂不再飞来 再也回不去的呀是教室里的挂钟老师曾经反复强调 篇四 清明 高一的时候，清明节要出去玩，就在迈出家门的前一步，月考成绩出来了。。。高中再无清明节 亏欠故乡一个清明那年许好的承诺待到闲时一定要将清明的春色踏遍 那年我就从故乡飞了出去飞到了一个四季如夏的南方花草不是故乡的树木不是故乡的就连从北方吹来的风也变了味道 故乡的清明啊，再遇你时你若还是识得我请迎我以明媚春光请迎我以纷纷细雨 篇五 远行 16 年春节，一个人坐火车回海南，路上发生的事 醒来的第一句竟是家乡话半路上来的师傅没能听懂 原来我已离家千万里 章五篇一 诗三百 诗三百新解 你如星子划过美丽了我的回忆 别离之后我用三百诗篇洒满了夏夜演绎着你的昔日我的昨天 篇二 高二，春天，倒春寒，学校的花大面积阵亡 那两天春风渐暖暖得让人醉醉到花红柳也绿绿了千里河山山上有翠柳青枝枝头有莺歌燕舞 东风易变变料峭春寒寒风过处处处花残 篇三 同上篇 在春风刚来的日子里暖风吹得杨柳绿 百花开温柔的抚摸着荒凉了一个季节的田野村前刚化冻的河边野鸭正成群游过 可等时令一变都跟着变了模样风不再温柔光不再明媚 只是可怜胸膛这颗不分季候的心还那般炽热 篇四 同窗 大一，同窗换了学校复读，梦里在母校碰到了他 不应该遇见你的你我都已经离去 是你意外的出现还是我忽然地闯入 这次不是命运的安排是我一个小小的梦 篇五 这篇太矫情了。。。 不愿意醉去怕遗漏你温柔的话语醒时只剩满天风和雨 不愿意哭泣怕模糊我最后的记忆今日你是如此的美丽 亭外柳花年年枝无数不愿折柳相赠远行人怕柳色青青不敌关山万里失颜色 来生怎舍得与你相遇怕山盟海誓不堪世事变易伤别离 章六篇一 我很后悔去海南读书，有些遗憾此生都难以弥补 以前想看的海，看一次就够了以前想走的山，走一次就累了舅舅家里的无花果熟了没庭院里不知名的植物还没有开花 儿时多少的欢乐都交付给了昨日那个从外面捎来奇异的水果的人也走了没有人再给我带来那伴随我长大的玩具 知道到了天涯海角的距离曾经的那个男儿志在四方现在不想去远的地方闯荡 篇二你不知道有个人还在为你伤心你不知道有个人还在为你哭泣 岁月轮回 日夜更替千呼万唤也唤不回你 想让风吹尽思念想让雪掩埋回忆最想让时光倒流","link":"/article/poems/"},{"title":"The tutorial of Hexo and Icarus theme","text":"My Customhexo-theme-icarus/zhangxu compare changes fix back-to-top bug add some icons add 163 music distinguish between thumbnail and cover modify profile layout add dropdown for toc … Quick StartInstall1npm install -g hexo SetupCreate Blog Project12345hexo init [blog_name]cd [blog_name]git initgit add .git commit -m 'init blog' Install Theme1git submodule add https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus Run1hexo s Deploy1234567891011# Remove generated files and cache.hexo clean# Generate static files.hexo generate# Deploy your website.hexo deploy# in shorthexo c &amp;&amp; hexo g &amp;&amp; hexo g Command12345678910111213141516Usage: hexo &lt;command&gt;Commands: clean Remove generated files and cache. config Get or set configurations. deploy Deploy your website. generate Generate static files. help Get help on a command. init Create a new Hexo folder. list List the information of the site migrate Migrate your site from other system to Hexo. new Create a new post. publish Moves a draft post from _drafts to _posts folder. render Render files with renderer plugins. server Start the server. version Display version information. ConfigGet or set configurations. 123Arguments: name Setting name. Leave it blank if you want to show all configurations. value New value of a setting. Leave it blank if you just want to show a single configuration. DeployDeploy your website. 123Options: --setup Setup without deployment -g, --generate Generate before deployment GenerateGenerate static files. 123456Options: -b, --bail Raise an error if any unhandled exception is thrown during generation -c, --concurrency Maximum number of files to be generated in parallel. Default is infinity -d, --deploy Deploy after generated -f, --force Force regenerate -w, --watch Watch file changes InitCreate a new Hexo folder at the specified path or the current directory. 123456Arguments: destination Folder path. Initialize in current folder if not specifiedOptions: --no-clone Copy files instead of cloning from GitHub --no-install Skip npm install ListList the information of the site. 12Arguments: type Available types: page, post, route, tag, category MigrateMigrate your site from other system to Hexo. 12Arguments: type Migrator type. NewCreate a new post. 12345678Arguments: layout Post layout. Use post, page, draft or whatever you want. title Post title. Wrap it with quotations to escape.Options: -p, --path Post path. Customize the path of the post. -r, --replace Replace the current post if existed. -s, --slug Post slug. Customize the URL of the post. PublishMoves a draft post from _drafts to _posts folder. 123Arguments: filename Draft filename. \"hello-world\" for example. layout Post layout. Use post, page, draft or whatever you want. RenderRender files with renderer plugins (e.g. Markdown) and save them at the specified path. 1234Options: --engine Specify render engine --output Output destination. Result will be printed in the terminal if the output destination is not set. --pretty Prettify JSON output ServerStart the server and watch for file changes. 123456Options: -i, --ip Override the default server IP. Bind to all IP address by default. -l, --log [format] Enable logger. Override log format. -o, --open Immediately open the server url in your default web browser. -p, --port Override the default port. -s, --static Only serve static files. Global Options1234567Global Options: --config Specify config file instead of using _config.yml --cwd Specify the CWD --debug Display all verbose messages in the terminal --draft Display draft posts --safe Disable all plugins and scripts --silent Hide output on console The directory structure12345678910111213├── _config.yml├── scaffolds│ ├── draft.md│ ├── page.md│ └── post.md├── source│ ├── _data│ ├── _drafts│ ├── _posts│ └── static└── themes ├── icarus └── landscape Configuration and Othershexo document icarus document Custom domaincreate a file named CNAME, fill blog.adream.win in it: 1echo blog.adream.win &gt; CNAME 1url: blog.adream.win github will generate certificates for your domain within 24 hours and then you can access blog.adream.win with https. IssueNeed to change the github OAuth Apps callback address after customizing the domain name, if you ever use it: gitalk cannot create issue SEOGoogleGoogle Search Console Baidu[Baidu]","link":"/tutorial/The-tutorial-of-hexo-and-Icarus-theme/"},{"title":"PostgreSQL 迁移数据","text":"记录 postgresql 数据库迁移中遇见的一个问题。 web服务使用的腾讯云 serverless 服务，在重新部署的过程中提示： 1警告：你正在使用Serverless Component的beta测试版本。请使用正式的版本以获得更多的特性，详情请见：https://github.com/serverless/components/blob/master/README.cn.md 经过查询发现，自己使用的 component 还是 v1 版本，v2 版本已经发布，在升级 postgresql 的过程中需要手动迁移数据库。 数据库备份：1pg_dump -U username -h hostname -p port databasename -f filename.sql 数据库导入：1psql -U username -h hostname -d desintationdb -p port -f filename.sql 在导入数据库之后，发现 web 服务不可用且 python manage.py migrate 也报错： 12345# error 1psycopg2.errors.UndefinedTable: relation \"user_wxmodel\" does not exist...# error 2psycopg2.errors.InvalidSchemaName: no schema has been selected to create in... 解决：在 pg_dump 备份文件中，注释掉下面这行即可： 1SELECT pg_catalog.set_config('search_path', '', false); 原因：no schema has been selected to create in … error","link":"/bug/pg%E8%BF%81%E7%A7%BB/"}],"tags":[{"name":"论文","slug":"论文","link":"/tags/%E8%AE%BA%E6%96%87/"},{"name":"异常检测","slug":"异常检测","link":"/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"},{"name":"综述","slug":"综述","link":"/tags/%E7%BB%BC%E8%BF%B0/"},{"name":"城市计算","slug":"城市计算","link":"/tags/%E5%9F%8E%E5%B8%82%E8%AE%A1%E7%AE%97/"},{"name":"中国科学院大学","slug":"中国科学院大学","link":"/tags/%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"现代诗","slug":"现代诗","link":"/tags/%E7%8E%B0%E4%BB%A3%E8%AF%97/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"}],"categories":[{"name":"教程","slug":"教程","link":"/categories/%E6%95%99%E7%A8%8B/"},{"name":"论文","slug":"论文","link":"/categories/%E8%AE%BA%E6%96%87/"},{"name":"城市计算","slug":"论文/城市计算","link":"/categories/%E8%AE%BA%E6%96%87/%E5%9F%8E%E5%B8%82%E8%AE%A1%E7%AE%97/"},{"name":"开源项目","slug":"开源项目","link":"/categories/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/"},{"name":"Docker","slug":"教程/Docker","link":"/categories/%E6%95%99%E7%A8%8B/Docker/"},{"name":"异常检测","slug":"论文/城市计算/异常检测","link":"/categories/%E8%AE%BA%E6%96%87/%E5%9F%8E%E5%B8%82%E8%AE%A1%E7%AE%97/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"},{"name":"杂记","slug":"杂记","link":"/categories/%E6%9D%82%E8%AE%B0/"},{"name":"博客开发","slug":"教程/博客开发","link":"/categories/%E6%95%99%E7%A8%8B/%E5%8D%9A%E5%AE%A2%E5%BC%80%E5%8F%91/"},{"name":"数据库,PostgreSQL,Bug","slug":"数据库-PostgreSQL-Bug","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93-PostgreSQL-Bug/"}]}