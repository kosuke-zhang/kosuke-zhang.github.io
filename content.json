{"pages":[{"title":"","text":"版本控制无论项目大小，我认为但凡属于编程的范畴，你都应该使用版本控制系统，这个编程本身的特点有很大的关系。版本控制系统有很多，其中 git 和 svn 名气和影响比较大，就目前而言 git 是最好的选择，没有之一。 我这里不再累述，如果你有些疑问，下面两篇文章或许可以帮助到你： 为什么要使用版本控制系统？ 为什么选择 Git？ 对 git 还没有任何了解？廖雪峰的教程还是很不错的，可以简要的阅读下： Git 教程 Github 的使用创建仓库 如果是一个人开发，可以直接在 master 分支下修改提交代码，不过规范的做法是要创建新的分支： 123456git branch dev # 创建 dev 分支git checkout dev # 切换到 dev 分支# orgit branch -b dev # 等价于上面的两条命令 在本地开发并测试结束以后，需要将 dev 分支 push 到 Github，再通过创建拉取请求 new pull request 的方式和 master 分支进行合并。 为什么要这么麻烦呢？ 两个原因： 如果我们创建了 CI，在提交合并请求的时候会自动触发 CI 来完成你提前规定的任务，比如代码测试、自动部署等，这使得我们在合并代码的时候就能完成很多任务，十分的便捷。 关于持续集成 创建 pull request 之后可以指定队友帮你 review，可以在一定程度上避免严重的 bug，保证代码质量。 Scrapy 开发Scrapy 很简单，也很强大。。。 Scrapy 用默认的配置就可以很好地完成任务，如果不符合需求，也很方便的对各种模块进行自定义。 定义 Item首先需要对保存的数据内容进行定义 名称 含义 id 每条数据的唯一标识符 news_id 新闻的唯一标识符 url 新闻地址 content 新闻内容 category 新闻类别 source 新闻来源 data 新闻日期 page 当前页数 entry_time 入库时间 中国公安网上一则新闻可能分为多页显示，我们对每一页分开进行保存，因为如果合并保存的话有可能页面太长导致存储错误。 id 是 url 和 method 的 sha1 值。 123from scrapy.utils.request import request_fingerprintid = request_fingerprint(response.request) 解析页面一共有 25 个类别的新闻数据需要抓取，这也新闻页面之间是有差异的，如果分开编写解析函数工作量太大，在对页面进行分析之后找到了很多共同点，很多数据可以使用相同 Xpath 进行提取。 数据项 Xpath title //*[@id=”newslist”]/h1/gettitle/text() content //*[@id=”fz_test”]/div[1]/table category //*[@id=”source_report”]/text() date //*[@id=”pub_time_report”]/text() 在提取总页数和当前页数的时候， 我们需要从 url 里面提取 news_id，url 有两种不同的格式，需要用不同的正则表达式进行分别处理： 12345# http://zhian.cpd.com.cn/n26237006/202001/t20200120_877942.htmlp_path1 = re.compile('(.*/)(.*?_.*?)\\.html')# 'http://jt.cpd.com.cn/n462015/c4191056/content.html'p_path2 = re.compile('(.*?)content.html') 在提取新闻的总页数和当前页数的时候有两种不同页面，也需要用两种不同的正则表达书进行处理： 12345# http://jt.cpd.com.cn/n462015/c4191056/content.htmlp_news1 = re.compile('createPageHTML\\((\\d+), (\\d+),')# 'http://minsheng.cpd.com.cn/n1448492/c3834444/content.html'p_news2 = re.compile('var maxPageNum=(\\d+);.*?var pageName = (\\d+);', re.S) 持久化在获取到数据之后存入到 MySQL 之中，数据表的定义如下： 12345678910111213141516171819202122create schema if not exists news collate utf8mb4_unicode_ci;use news;create table if not exists cpd_news( id varchar(40) not null, url varchar(255) not null, title varchar(255) not null, content text not null, category varchar(5) not null, source varchar(50) not null, date varchar(30) not null, news_id varchar(50) not null, page int not null, entry_time DATETIME not null DEFAULT CURRENT_TIMESTAMP, constraint data_id_uindex unique (id));alter table data add primary key (id); 利用 PyMysql 库与 MySQL 进行交互。 URL 去重每次在启动爬虫的时候，已经抓取的 URL 可以直接跳过不必再重复抓取，Scrapy 提供了去重的方法，但是默认的情况下 URL 的持久化数据存在在了本地，难以迁移，而且请求失败的 URL 也同样被存储了起来，这不符合我们的要求，我们重新定义了 DUPEFILTER_CLASS 去重方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# settings.pyDUPEFILTER_CLASS = 'crawler.dupefilters.RFPDupeFilter'# dupefilters.pyclass RFPDupeFilter(BaseDupeFilter): \"\"\"Request Fingerprint duplicates filter\"\"\" def __init__(self, database_name=None, table_name=None, filter_name=None, debug=False): self.fingerprints = set() self.logdupes = True self.debug = debug self.logger = logging.getLogger(__name__) self.fingerprints.update() if database_name and table_name: host = settings.get('MYSQL_HOST', 'localhost') mysql_user = settings.get('MYSQL_USER', 'root') mysql_pwd = settings.get('MYSQL_PASSWORD', 'news_crawler') mysql_port = settings.get('MYSQL_PORT', 3306) self.db = pymysql.connect(host, mysql_user, mysql_pwd, database_name, mysql_port) self.cursor = self.db.cursor() sql = \"SELECT {0} FROM {1} WHERE 1\".format(filter_name, table_name) self.cursor.execute(sql) ids = self.cursor.fetchall() ids = map(lambda x: x[0], ids) self.fingerprints.update(ids) @classmethod def from_crawler(cls, crawler): debug = settings.getbool('DUPEFILTER_DEBUG') return cls(crawler.spider.database_name, crawler.spider.table_name, crawler.spider.filter_name, debug) def request_seen(self, request): fp = self.request_fingerprint(request) if fp in self.fingerprints: return True self.fingerprints.add(fp) def request_fingerprint(self, request): return request_fingerprint(request) def close(self, reason): if self.db and self.cursor: self.db.close() self.cursor.close() def log(self, request, spider): if self.debug: msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\" args = {'request': request, 'referer': referer_str(request)} self.logger.debug(msg, args, extra={'spider': spider}) elif self.logdupes: msg = (\"Filtered duplicate request: %(request)s\" \" - no more duplicates will be shown\" \" (see DUPEFILTER_DEBUG to show all duplicates)\") self.logger.debug(msg, {'request': request}, extra={'spider': spider}) self.logdupes = False spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider) 可以看到，我们在每次启动爬虫的时候从数据库获取 URL 信息，对即将抓去的链接进行检测去重，相对于默认的去重模式，我们重写后的方法不再存在难以迁移的问题，也能更加方便的进行管理。 反爬添加代理由于对方服务器的反爬措施比较严格，添加 IP 代理是最便捷也是最有效的突破反爬的方式。我们使用的是阿布代理，每秒钟最多可以发送 5 次请求，每次请求的代理 IP 会随机变化，所以需要同时限制抓取频率： 123456789101112131415161718192021222324# 限制抓取频率# settings.pyCONCURRENT_REQUESTS = 5DOWNLOAD_DELAY = 0.2RANDOMIZE_DOWNLOAD_DELAY = False# 添加代理# middlewares.py# 代理服务器proxyServer = \"http://http-dyn.abuyun.com:9020\"# 代理隧道验证信息proxyUser = \"\"proxyPass = \"\"proxyAuth = \"Basic \" + base64.urlsafe_b64encode(bytes((proxyUser + \":\" + proxyPass), \"ascii\")).decode(\"utf8\")class ProxyMiddleware(object): def process_request(self, request, spider): request.meta[\"proxy\"] = proxyServer request.headers[\"Proxy-Authorization\"] = proxyAuth 随机更换 User-Agent检测 User-Agent 是反爬的重要手段之一，为了躲避这种反爬手段，我们需要随机更换 User-Agent： 12345678910111213141516171819202122232425262728293031323334# 随机更换 User-Agent# middlewares.pyclass RandomUserAgentMiddleware(object): \"\"\"This middleware allows spiders to override the user_agent\"\"\" def __init__(self): self.user_agent_list = settings.get('USER_AGENT_LIST') self.count = 0 def process_request(self, request, spider): request.headers['User-Agent'] = random.choice(self.user_agent_list)# User-Agent 列表# settings.pyUSER_AGENT_LIST = [ 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50', 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50', 'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0', 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)', 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)', 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1', 'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1', 'Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11', 'Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11', 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)', 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)', 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)', 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)', 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)', 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',] 部署Docker Build使用 Docker，可以简化部署流程，更轻松的迁移以及更轻松的维护和扩展。 为什么要用 Docker 1234567891011FROM python:3.7.3RUN mkdir /projectWORKDIR /projectADD crawler/. /projectRUN mkdir error logRUN pip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements.txt Docker-compose使用 docker-compose 分别部署数据库和爬虫程序。 数据库： 1234567891011121314151617181920212223242526272829303132333435version: '3'services: db: image: mysql container_name: mysql-crawler command: mysqld --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci #设置utf8字符集 restart: always environment: MYSQL_ROOT_PASSWORD: news_crawler ports: - '3306:3306' volumes: - ../data/mysql/db:/var/lib/mysql - ./init/cpd.sql:/docker-entrypoint-initdb.d/init.sql:ro networks: - crawler_db gui: depends_on: - db image: phpmyadmin/phpmyadmin container_name: phpmyadmin-crawler restart: always environment: MYSQL_ROOT_PASSWORD: news_crawler PMA_HOST: db ports: - '8000:80' networks: - crawler_dbnetworks: crawler_db: 爬虫： 123456789101112131415version: '3'services: crawler: image: registry.cn-hangzhou.aliyuncs.com/traffic_news/cpd_crawler:latest container_name: crawler command: scrapy crawl cpd volumes: - ./log/:/project/log/ networks: - crawler_netnetworks: crawler_net: 自动化构建 Docker 镜像在每次将代码提交合并到 github 的 master 分支上之后，自动的构建 docker image 并自动上传到镜像仓库。 使用阿里云的容器镜像服务，创建自动构建设置，当项目 mater 分支更新则自动构建并上传镜像仓库。 定时抓取利用 Linux crontab 服务，创建定时任务： 1crontab -e # 打开服务设置 设置自动定时任务 110,20,30,40,50,59 * * * * /usr/local/bin/docker-compose -f /root/Projects/traffic_news/deploy/docker-compose-crawler.yml up -d 重新加载 crontab，生效服务 1service cron reload","link":"/Untitled.html"}],"posts":[{"title":"About me","text":"独立人格 自由思想 Bilibili 转发投币点赞了解一下？ /* scale 用来实现宽高等比例 */ .scale { width: 100%; padding-bottom: 56.25%; height: 0; position: relative; } /* item 用来放置全部的子元素 */ .item { width: 100%; height: 100%; position: absolute; } 中国科学院大学 人工智能学院 硕士 在读 自诩是一个会技术的产品。 那些疯狂到以为自己能够改变世界的人，才能真正改变世界。我一定会倒在让这个世界更加美好的路上。 实习经历 计算所泛在研究中心 BDA 投资有限公司技术部 北京长亭科技安全策略部","link":"/2019/11/12/about-me/"},{"title":"UCAS Affair Scripts","text":"项目地址 开发/收集 中国科学院大学 各种事务的自动化脚本 已有脚本： 选课 监听成绩 期待贡献，添加更多有趣的脚本，kill TodoList item Warning: 由于 选课系统 只能单点登录，如果同时运行两个脚本，或者在运行脚本的时候登陆网页端，他们之前会争抢会话。 脚本默认在会话失效后再次自动获取，所以在刷课和监听成绩的过程中，如果您需要浏览选课系统的网页端，需要先暂停脚本。 script12kill -STOP $pid # suspendkill -CONT $pid # resume UsageInitializationPull project: script1git clone https://github.com/zhangxu3486432/UCAS-Affair-Scripts.git Enter the working directory: script1cd UCAS-Affair-Scripts Install dependency: script1pip3 install -r deploy/requirements.txt Configuration Set in the settings.py Set http://sep.ucas.ac.cn/ UserName and PassWord 12USERNAME = ''PASSWORD = '' Login In the off-campus network login need to fill in the Verification Code. First login to save cookies in sep.cookie, so that you can facilitate deployment in the server. script1python3 login.py The Verification Code picture is saved in verification.png, open the picture identification key and fill it in terminal. As you can see, this Verification Code is 7351. Fill the 7351 in terminal and press enter. RUN take-courses Set the courses which you want to take Examples: 1COURSES = ['自然语言处理', '机器人智能控制', '积极心理学'] Fill in the college to which the course you choose belongs Examples: 1COLLEGES = ['人工智能学院', '心理学系'] Run script1python3 take_courses.py Run monitor_grades Set email info if you want to monitor your grads, you need to set it 1234SEND_EMAIL = ''SEND_EMAIL_PWD = ''RECEIVE_EMAIL = '' Run script1python3 monitor_grades.py TodoList Test and adapt to more python versions Shuttle Bus Reservation Build Docker Image Automatic Identification Verification Code","link":"/2020/02/04/software-2020-02-04-UCAS-Affair-Scripts/"},{"title":"The tutorial of Hexo and Icarus theme","text":"My Customhexo-theme-icarus/zhangxu compare changes fix back-to-top bug add some icons add 163 music distinguish between thumbnail and cover modify profile layout add dropdown for toc … Quick StartInstall1npm install -g hexo SetupCreate Blog Project12345hexo init [blog_name]cd [blog_name]git initgit add .git commit -m 'init blog' Install Theme1git submodule add https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus Run1hexo s Deploy1234567891011# Remove generated files and cache.hexo clean# Generate static files.hexo generate# Deploy your website.hexo deploy# in shorthexo c &amp;&amp; hexo g &amp;&amp; hexo g Command12345678910111213141516Usage: hexo &lt;command&gt;Commands: clean Remove generated files and cache. config Get or set configurations. deploy Deploy your website. generate Generate static files. help Get help on a command. init Create a new Hexo folder. list List the information of the site migrate Migrate your site from other system to Hexo. new Create a new post. publish Moves a draft post from _drafts to _posts folder. render Render files with renderer plugins. server Start the server. version Display version information. ConfigGet or set configurations. 123Arguments: name Setting name. Leave it blank if you want to show all configurations. value New value of a setting. Leave it blank if you just want to show a single configuration. DeployDeploy your website. 123Options: --setup Setup without deployment -g, --generate Generate before deployment GenerateGenerate static files. 123456Options: -b, --bail Raise an error if any unhandled exception is thrown during generation -c, --concurrency Maximum number of files to be generated in parallel. Default is infinity -d, --deploy Deploy after generated -f, --force Force regenerate -w, --watch Watch file changes InitCreate a new Hexo folder at the specified path or the current directory. 123456Arguments: destination Folder path. Initialize in current folder if not specifiedOptions: --no-clone Copy files instead of cloning from GitHub --no-install Skip npm install ListList the information of the site. 12Arguments: type Available types: page, post, route, tag, category MigrateMigrate your site from other system to Hexo. 12Arguments: type Migrator type. NewCreate a new post. 12345678Arguments: layout Post layout. Use post, page, draft or whatever you want. title Post title. Wrap it with quotations to escape.Options: -p, --path Post path. Customize the path of the post. -r, --replace Replace the current post if existed. -s, --slug Post slug. Customize the URL of the post. PublishMoves a draft post from _drafts to _posts folder. 123Arguments: filename Draft filename. \"hello-world\" for example. layout Post layout. Use post, page, draft or whatever you want. RenderRender files with renderer plugins (e.g. Markdown) and save them at the specified path. 1234Options: --engine Specify render engine --output Output destination. Result will be printed in the terminal if the output destination is not set. --pretty Prettify JSON output ServerStart the server and watch for file changes. 123456Options: -i, --ip Override the default server IP. Bind to all IP address by default. -l, --log [format] Enable logger. Override log format. -o, --open Immediately open the server url in your default web browser. -p, --port Override the default port. -s, --static Only serve static files. Global Options1234567Global Options: --config Specify config file instead of using _config.yml --cwd Specify the CWD --debug Display all verbose messages in the terminal --draft Display draft posts --safe Disable all plugins and scripts --silent Hide output on console The directory structure12345678910111213├── _config.yml├── scaffolds│ ├── draft.md│ ├── page.md│ └── post.md├── source│ ├── _data│ ├── _drafts│ ├── _posts│ └── static└── themes ├── icarus └── landscape Configuration and Othershexo document icarus document","link":"/2019/11/12/tutorial-The-tutorial-of-hexo-and-Icarus-theme/"},{"title":"城市异常事件检测","text":"","link":"/2019/11/23/paper-%E5%9F%8E%E5%B8%82%E5%BC%82%E5%B8%B8%E4%BA%8B%E4%BB%B6%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"中国科学院大学","slug":"中国科学院大学","link":"/tags/%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"论文","slug":"论文","link":"/tags/%E8%AE%BA%E6%96%87/"},{"name":"异常检测","slug":"异常检测","link":"/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"},{"name":"综述","slug":"综述","link":"/tags/%E7%BB%BC%E8%BF%B0/"},{"name":"城市计算","slug":"城市计算","link":"/tags/%E5%9F%8E%E5%B8%82%E8%AE%A1%E7%AE%97/"}],"categories":[{"name":"教程","slug":"教程","link":"/categories/%E6%95%99%E7%A8%8B/"},{"name":"关于我","slug":"关于我","link":"/categories/%E5%85%B3%E4%BA%8E%E6%88%91/"},{"name":"开源项目","slug":"开源项目","link":"/categories/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/"},{"name":"博客开发","slug":"教程/博客开发","link":"/categories/%E6%95%99%E7%A8%8B/%E5%8D%9A%E5%AE%A2%E5%BC%80%E5%8F%91/"},{"name":"论文","slug":"论文","link":"/categories/%E8%AE%BA%E6%96%87/"},{"name":"城市计算","slug":"论文/城市计算","link":"/categories/%E8%AE%BA%E6%96%87/%E5%9F%8E%E5%B8%82%E8%AE%A1%E7%AE%97/"},{"name":"异常检测","slug":"论文/城市计算/异常检测","link":"/categories/%E8%AE%BA%E6%96%87/%E5%9F%8E%E5%B8%82%E8%AE%A1%E7%AE%97/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"}]}