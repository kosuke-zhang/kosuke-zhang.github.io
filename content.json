{"pages":[{"title":"","text":"404 您访问的资源不存在，正在前往网站首页... setTimeout(() => { window.location.href = \"https://blog.adream.win/\"; }, 1000);","link":"/404.html"},{"title":"","text":"手写数字识别加载数据torchvision datasets 提供了 minist 数据集下载 1234567891011121314train_data = dsets.MNIST( root='./mnist/', train=True, transform=transforms.ToTensor(), download=True,)train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)test_data = dsets.MNIST(root='./mnist/', train=False, transform=transforms.ToTensor())text_x1 = test_data.data[:2000]text_x2 = test_data.data[:2000]test_x = torch.cat((text_x1, text_x2.permute(0, 2, 1)), dim=1) / 255.test_y = test_data.targets[:2000] 构建网络使用双向多层 LSTM 对图像进行处理，串联第 n 行，第 n 列的像素作为第 n 个时间步的输入。 12345678910111213141516171819202122232425262728class RNN(nn.Module): def __init__(self, input_size, hidden_size, n_class, num_layers, dropout, bidirectional): super(RNN, self).__init__() self.rnn = nn.LSTM( input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional, ) L_size = hidden_size * 2 if bidirectional else hidden_size self.out = nn.Linear(L_size, n_class) def forward(self, x): if not hasattr(self, '_flattened'): self.rnn.flatten_parameters() setattr(self, '_flattened', True) # x shape (batch, time_step, input_size) # r_out shape (batch, time_step, output_size) # h_n shape (n_layers, batch, hidden_size) # h_c shape (n_layers, batch, hidden_size) r_out, (h_n, c_n) = self.rnn(x, None) out = self.out(r_out.mean(1)) return out 训练过程损失函数采用的是 CrossEntropyLoss，优化器采用的 Adam。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455model = RNN( input_size=INPUT_SIZE, hidden_size=hidden, n_class=N_CLASS, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)loss_func = nn.CrossEntropyLoss()model = nn.DataParallel(model, device_ids=[0, 1, 2, 3, 4])model.cuda()test_x = test_x.cuda()test_y = test_y.cuda()x = []train_acc_set = []loss_set = []test_acc_set = []step = 0for epoch in range(epochs): for b_x, b_y in train_loader: input1 = b_x.view(-1, 28, 28) input2 = b_x.permute(0, 1, 3, 2).view(-1, 28, 28) input = torch.cat((input1, input2), dim=1) input = input.cuda() b_y = b_y.cuda() output = model(input) pred_y = torch.max(output, 1)[1] loss = loss_func(output, b_y) optimizer.zero_grad() loss.backward() optimizer.step() if step % 50 == 0: x.append(step) train_acc = float((pred_y == b_y).type(torch.int).sum()) / float(len(b_y)) train_acc_set.append(train_acc) loss_set.append(loss.item()) test_output = model(test_x) pred_y = torch.max(test_output, 1)[1] test_acc = float((pred_y == test_y).type(torch.int).sum()) / float(len(test_y)) test_acc_set.append(test_acc) print('Epoch: ', epoch, 'step: ', step, '| train loss: %.4f' % loss.item(), '| train accuracy: %.2f' % train_acc, '| test accuracy: %.2f' % test_acc) step += 1 实验分析经过 10 个 epochs，最终的结果稳定在 0.97，这表明 LSTM 也有提取并记忆图像特征的能力。 猫狗分类加载数据首先对原始数据进行处理，在原始数据中 cat 和 dog 的图片都存放在一个文件夹内，首先将 cat 和 dog 的图片存放在不同的文件夹 12345mkdir catmkdir dogmv cat.*.jpg ./catmv dog.*.jpg ./dog 再利用 ImageFolder 函数加载数据 12345678910111213normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])transform = transforms.Compose([ transforms.Resize((227, 227)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize,])train_dataset = ImageFolder('/home/zx19/projects/cat_dog/dataset/train/',transform=transform)trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)validation_dataset = ImageFolder('/home/zx19/projects/cat_dog/dataset/validation/',transform=transform)validationloader = torch.utils.data.DataLoader(validation_dataset, batch_size=500, shuffle=True) 构建网络采用的是 AlexNet 构建的网络 1234567891011121314151617181920212223242526272829303132333435363738394041424344class AlexNet(nn.Module): def __init__(self, num_classes=1000): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) self.classifier = nn.Sequential( nn.Dropout(), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.classifier(x) return xdef alexnet(pretrained=False, progress=True, **kwargs): model = AlexNet(**kwargs) if pretrained: state_dict = torch.hub.load_state_dict_from_url('https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth', progress=progress) model.load_state_dict(state_dict) return model 在 alexnet 函数的参数 pretrained 为 True 时，将加载 ImageNet 的预训练模型 初始化参数1234567891011121314151617def weights_init(m): classname = m.__class__.__name__ if classname.find('Conv2d') != -1: m.weight.data.normal_(0.0, 0.02) elif classname.find('BatchNorm') != -1: m.weight.data.normal_(1.0, 0.02) m.bias.data.fill_(0) elif classname.find('Linear') != -1: nn.init.xavier_uniform_(m.weight) m.bias.data.fill_(0)model = alexnet(pretrained=False)model.classifier[6] = nn.Linear(4096, 2)alexNet = nn.DataParallel(model, device_ids=[0, 1, 2, 3, 4])alexNet = alexNet.cuda()alexNet.apply(weights_init) 训练过程损失函数采用的是 CrossEntropyLoss，优化器使用的是 SGD 123456789101112131415161718192021222324252627282930313233343536373839404142434445criterion = nn.CrossEntropyLoss()optimizer = optim.SGD(alexNet.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)epochs = 10train_losses, validation_losses = [], []loss_step = 0for e in range(epochs): for step, (images,labels) in enumerate(trainloader): images = images.cuda() labels = labels.cuda() optimizer.zero_grad() outputs = alexNet(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() loss_step += loss.item() if (step+1)%10==0: with torch.no_grad(): for images, labels in validationloader: images = images.cuda() labels = labels.cuda() outputs = alexNet(images) validation_loss = criterion(outputs, labels) equals = outputs.max(1).indices == labels accuracy = torch.mean(equals.type(torch.float)) break train_losses.append(loss.item()) validation_losses.append(validation_loss) print(&quot;Epoch: {}/{}&quot;.format(e+1, epochs), &quot; | Training Loss: {:.3f}&quot;.format(loss.item()), &quot; | Test Loss: {:.3f}&quot;.format(validation_loss.item()), &quot; | Test Accuracy: {:.3f}&quot;.format(accuracy)) 实验分析 在经过 10 个 epochs 训练后，准确率到达了 85%。期间 train loss 和 validation loss 都在持续下降，这说明增加 epochs 还有提高准确率的空间。 自动写诗加载数据1234567datas = np.load(\"/home/zx19/projects/poem/tang.npz\", allow_pickle=True)data = datas['data']index2word = datas['ix2word'].item()word2index = datas['word2ix'].item()data = torch.from_numpy(data)train_loader = DataLoader(data, batch_size=128, shuffle=True, num_workers=2) 构建网络使用了多层单向 LSTM 网络进行训练 1234567891011121314151617181920212223242526272829class Poem(nn.Module): def __init__(self, vocab_size, embedding_dim, hidden_dim): super(Poem, self).__init__() self.num_layers = 2 self.hidden_dim = hidden_dim self.embeddings = nn.Embedding(vocab_size, embedding_dim) #vocab_size:就是ix2word这个字典的长度。 self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=self.num_layers, batch_first=True,dropout=0, bidirectional=False) self.fc1 = nn.Linear(self.hidden_dim,2048) self.fc2 = nn.Linear(2048,4096) self.fc3 = nn.Linear(4096,vocab_size) def forward(self, input, hidden=None): if not hasattr(self, '_flattened'): self.lstm.flatten_parameters() setattr(self, '_flattened', True) embeds = self.embeddings(input) # [batch, seq_len] =&gt; [batch, seq_len, embed_dim] batch_size, seq_len = input.size() if hidden is None: h_0 = input.data.new(self.num_layers*1, batch_size, self.hidden_dim).fill_(0).float() c_0 = input.data.new(self.num_layers*1, batch_size, self.hidden_dim).fill_(0).float() else: h_0, c_0 = hidden output, hidden = self.lstm(embeds, (h_0, c_0))#hidden 是h,和c 这两个隐状态 output = torch.tanh(self.fc1(output)) output = torch.tanh(self.fc2(output)) output = self.fc3(output) output = output.reshape(batch_size * seq_len, -1) return output,hidden 训练过程损失函数使用的是 CrossEntropyLoss，优化器采用的是 Adam 12345678910111213141516171819202122232425262728293031323334epochs = 5model = Poem(len(word2index), embedding_dim=100, hidden_dim=1024)model = model.to(device)optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)criterion = nn.CrossEntropyLoss()model.train()for epoch in range(epochs): train_loss = 0 train_acc = 0 i = 0 for step, data in enumerate(train_loader): i += 1 inputs, labels = data[:, :-1].to(device).to(torch.long), data[:, 1:].to(device).to(torch.long) labels = labels.view(-1) optimizer.zero_grad() outputs,hidden = model(inputs) loss = criterion(outputs,labels) loss.backward() optimizer.step() _,pred = outputs.topk(1) pred = pred.view(-1) acc = (pred==labels).to(torch.float).mean() train_acc+=acc train_loss += loss.item() if (step+1)%100==0: print('Epoch: {}/{} | Loss: {} | Acc: {}'.format(epoch+1, epochs, train_loss/i, train_acc/i)) i = 0 train_loss = 0 train_acc = 0 print('Epoch: {}/{} | Loss: {} | Acc: {}'.format(epoch+1, epochs, train_loss/i, train_acc/i)) 预测部分在生成下一个预测部分的使用，采样函数使用了 softmax temperature，并不是直接去概率最大的值，temperature 参数控制了随机性，是根据每个词的概率进行采样。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def generate(model, start_words, ix2word, word2ix): results = list(start_words) start_words_len = len(start_words) # 第一个词语是&lt;START&gt; input = t.Tensor([word2ix['&lt;START&gt;']]).view(1, 1).long() hidden = None model.eval() with torch.no_grad(): for i in range(Config.max_gen_len): output, hidden = model(input, hidden) # 如果在给定的句首中，input为句首中的下一个字 if i &lt; start_words_len: w = results[i] input = input.data.new([word2ix[w]]).view(1, 1) # 否则将output作为下一个input进行 else: top_index = output.data[0].topk(1)[1][0].item() w = ix2word[top_index] results.append(w) input = input.data.new([top_index]).view(1, 1) if w == '&lt;EOP&gt;': del results[-1] break return resultsdef sample(output, temperature): output = F.softmax(output.to(torch.float64) / temperature) # output 默认是 float 32，但是 np.random.multinomial 在处理时会将数据强制转换到 float 64，会出现精度上的损失 output = output.to('cpu').detach().numpy() return np.random.multinomial(1, output[0], 1) start_words = '雪'results = list(start_words)start_words_len = len(start_words)# 第一个词语是&lt;START&gt;input = torch.Tensor([word2index['&lt;START&gt;']]).view(1, 1).long().cuda()hidden = Nonemodel.eval()with torch.no_grad(): for i in range(125): output, hidden = model(input, hidden) # 如果在给定的句首中，input为句首中的下一个字 if i &lt; start_words_len: w = results[i] input = input.data.new([word2index[w]]).view(1, 1) # 否则将output作为下一个input进行 else: top_index = sample(output, 0.5) top_index = np.where(top_index[0]==1)[0].item() w = index2word[top_index] results.append(w) input = input.data.new([top_index]).view(1, 1) if w == '&lt;EOP&gt;': del results[-1] breakprint(''.join(results)) 实验分析12Input: 月Output: 月豺有性，近问东所。一声留夜，日起相关。 由于采用了 softmax temperature 采样下一个预测词，并没有出现生成和训练集里面的诗歌一模一样的情况。 电影评论情感分类加载数据导入训练数据，构建 MovieDataset，继承于 Dataset 123456789101112131415161718192021222324252627282930class MovieDataset(Dataset): def __init__(self, path, word_vecs, word2id): self.df = pd.read_csv(path, sep='\\t', header=None) self.labels = list(self.df[0]) self.sentences = list(self.df[1]) self.word_vecs = word_vecs self.word2id = word2id def __len__(self): return len(self.df) def __getitem__(self, index): label = self.labels[index] sentence = self.sentences[index] words = sentence.split(' ')[:75] sentence_vec = [] sentence_vec = torch.zeros(1, 75, 50) for i, word in enumerate(words): sentence_vec[0][i] = torch.tensor(self.word_vecs[self.word2id[word]]) return sentence_vec, labeltrain_dataset = MovieDataset('./dataset/train.txt', word_vecs, word2id)train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)validation_dataset = MovieDataset('./dataset/validation.txt', word_vecs, word2id)validation_dataloader = DataLoader(train_dataset, batch_size=len(validation_dataset))for validation_datas, validation_labels in validation_dataloader: validation_datas = validation_datas.cuda() validation_labels = validation_labels.cuda() 加载词向量以及构建 word2index 字典12345678910111213141516171819202122232425262728293031323334353637383940def build_word2id(path): \"\"\" :param file: word2id 保存地址 :return: None \"\"\" word2id = {'_PAD_': 0} for _path in path: with open(_path, encoding='utf-8') as f: for line in f.readlines(): sp = line.strip().split() for word in sp[1:]: if word not in word2id.keys(): word2id[word] = len(word2id) return word2iddef build_word2vec(fname, word2id, save_to_path=None): \"\"\" :param fname: 预训练的word2vec. :param word2id: 语料文本中包含的词汇集. :param save_to_path: 保存训练语料库中的词组对应的word2vec到本地 :return: 语料文本中词汇集对应的word2vec向量{id: word2vec}. \"\"\" n_words = max(word2id.values()) + 1 model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True) word_vecs = np.array(np.random.uniform(-1., 1., [n_words, model.vector_size])) for word in word2id.keys(): try: word_vecs[word2id[word]] = model[word] except KeyError: pass if save_to_path: with open(save_to_path, 'w', encoding='utf-8') as f: for vec in word_vecs: vec = [str(w) for w in vec] f.write(' '.join(vec)) f.write('\\n') return word_vecsword2id = build_word2id(['./dataset/train.txt', './dataset/validation.txt'])word_vecs = build_word2vec('./dataset/wiki_word2vec_50.bin', word2id) 网络模型12345678910111213141516171819202122232425262728293031class TextCNN(nn.Module): def __init__(self, word_embedding_dimension, sentence_max_size, label_num): super(TextCNN, self).__init__() self.label_num = label_num self.conv3 = nn.Conv2d(1, 1, (3, word_embedding_dimension)) self.conv4 = nn.Conv2d(1, 1, (4, word_embedding_dimension)) self.conv5 = nn.Conv2d(1, 1, (5, word_embedding_dimension)) self.Max3_pool = nn.MaxPool2d((sentence_max_size-3+1, 1)) self.Max4_pool = nn.MaxPool2d((sentence_max_size-4+1, 1)) self.Max5_pool = nn.MaxPool2d((sentence_max_size-5+1, 1)) self.linear1 = nn.Linear(3, label_num) def forward(self, x): batch = x.shape[0] # Convolution x1 = F.relu(self.conv3(x)) x2 = F.relu(self.conv4(x)) x3 = F.relu(self.conv5(x)) # Pooling x1 = self.Max3_pool(x1) x2 = self.Max4_pool(x2) x3 = self.Max5_pool(x3) x = torch.cat((x1, x2, x3), -1) x = x.view(batch, 1, -1) x = self.linear1(x) x = x.view(-1, self.label_num) return x 训练过程1234567891011121314151617181920212223242526272829303132333435363738394041424344batch_size=32epoch=10gpu=0out_channel=2label_num=2embed_dim = 50sentence_len = 75model = TextCNN(embed_dim, sentence_len, label_num)model = model.cuda()# model.apply(weights_init)criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)count = 0loss_sum = 0acc_sum = 0for epoch in range(epoch): for datas, labels in train_dataloader: datas = datas.cuda() labels = labels.cuda() outs = model(datas) loss = criterion(outs, labels) optimizer.zero_grad() loss.backward() optimizer.step() loss_sum += loss.item() count += 1 if count % 100 == 0: with torch.no_grad(): validation_outs = model(validation_datas) validation_loss = criterion(validation_outs, validation_labels) validation_acc = (validation_outs.topk(1)[1].view(-1)==validation_labels).to(torch.float).mean() print(\"Epoch: {} | train_loss: {:.3}, v_loss: {:.3}, v_acc: {:.3}\".format(epoch+1, loss_sum/100, validation_loss, validation_acc)) loss_sum = 0 count = 0 acc_sum = 0 实验分析12345Epoch: 8 | train_loss: 0.467, v_loss: 0.449, v_acc: 0.783Epoch: 8 | train_loss: 0.476, v_loss: 0.481, v_acc: 0.764Epoch: 8 | train_loss: 0.479, v_loss: 0.433, v_acc: 0.797Epoch: 8 | train_loss: 0.457, v_loss: 0.451, v_acc: 0.781Epoch: 8 | train_loss: 0.457, v_loss: 0.398, v_acc: 0.818 在第 8 个 epochs 的时候，在验证机上的准确率达到了 80%。textcnn 虽然说训练速度快，但是效果相比于 LSTM 较弱。","link":"/README.html"},{"title":"","text":"线程和进程的区别 进程是资源分配的最小单位，线程是CPU调度的最小单位 进程在执行的过程中，必须准备好上下文环境，也就是进程所需要的配到资源，这也是为什么进程之间难以共享内存，其实也没必要进行共享，因为我们可以预先对需要完成的任务进行分配，一个只执行其负责的任务。 一个进程可以有多个线程，线程之间是可以进行内存共享的，为了更有效地利用 CPU 资源，比如当一个线程正在等待网络请求，这个时候可以启动其他的线程，对还未完成的任务进行处理。 位置参数、args 可变参数、*kwargs 关键字参数、命名关键字参数 可变参数将被组合成 tuple，关键字参数会被组合成 dict。 关键字参数可以被用来扩展功能。 命名关键字参数限制了关键字参数的名字。 在Python中定义函数，可以用必选参数、默认参数、可变参数、关键字参数和命名关键字参数，这5种参数都可以组合使用。但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。 Python 内置容器及其使用场景 列表、元组、字典、集合 操作系统为什么有用户态和内核态，用户级线程与内核级线程如何转换 用户态内核态 中断 中断的本质是有高优先级或者突发的事件，需要在一定时间内响应。","link":"/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87.html"},{"title":"About me","text":"独立人格 自由思想 转发投币点赞了解一下？☞ ☞ ☞ Bilibili /* scale 用来实现宽高等比例 */ .scale { width: 100%; padding-bottom: 56.25%; height: 0; position: relative; } /* item 用来放置全部的子元素 */ .item { width: 100%; height: 100%; position: absolute; } 中国科学院大学 人工智能学院 硕士 在读 自诩是一个会技术的产品。 那些疯狂到以为自己能够改变世界的人，才能真正改变世界。我一定会倒在让这个世界更加美好的路上。 实习经历 计算所泛在研究中心 BDA 投资有限公司技术部 北京长亭科技安全策略部","link":"/about/index.html"}],"posts":[{"title":"PyG","text":"PyG 学习笔记 安装 确保你的 pytorch 至少是 1.4.0 以上的版本 12$ python -c \"import torch; print(torch.__version__)\"&gt;&gt;&gt; 1.6.0 查看和 pytorch 一起安装的 CUDA 的版本 12$ python -c \"import torch; print(torch.version.cuda)\"&gt;&gt;&gt; 10.2 安装与 pytorch 和 CUDA 对应的包 12345$ pip install torch-scatter==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-${TORCH}.html$ pip install torch-sparse==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-${TORCH}.html$ pip install torch-cluster==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-${TORCH}.html$ pip install torch-spline-conv==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-${TORCH}.html$ pip install torch-geometric ${CUDA} 应该被 CUDA 版本信息替换，例如：10.1: cu101， 10.2: cu102 ${TORCH} 应该被 Pytorch 版本信息替换，例如：1.4.0，1.50，1.6.0 当 PyTorch 1.6.0 and CUDA 10.2 时： 1234567$ CUDA=cu102$ TORCH=1.6.0$ pip install torch-scatter==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-${TORCH}.html$ pip install torch-sparse==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-${TORCH}.html$ pip install torch-cluster==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-${TORCH}.html$ pip install torch-spline-conv==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-${TORCH}.html$ pip install torch-geometric 常见错误 Frequently Asked Questions 介绍PyG 主要包括一下特征： Data Handling of Graphs Common Benchmark Datasets Mini-batches Data Transforms Learning Methods on Graphs 图数据处理 基准数据集 小批量 数据转换 图学习方法 图数据处理图模型被用来处理对象（nodes）之间的成对关系（edges）。一张图在 PyG 中用 torch_geometric.data.Data 实例来表示，默认有如下属性： data.x: 节点特征，形状：[num_nodes, num_node_features] data.edge_index: 图连通性 COO 格式的矩阵，形状：[2, num_edges]，数据类型 torch.long data.edge_attr: 边的特征，形状：[num_edges, num_edge_features] data.y: 训练目标，可以是任意形状, 节点级别目标：[num_nodes, *] 图级别目标： [1, *] data.pos: 节点位置矩阵：[num_nodes, num_dimensions] 这些属性是必须的，除此之外还可以进行扩展。 一个例子无方向、无权重、三个节点、四条边，每个节点包含一个特征： 12345678import torchfrom torch_geometric.data import Dataedge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)x = torch.tensor([[-1], [0], [1]], dtype=torch.float)data = Data(x=x, edge_index=edge_index) edge_index 定义了源和目标节点，并不是索引元组的列表，在使用索引列表之前应该先进行转置和 contiguous： 12345678910import torchfrom torch_geometric.data import Dataedge_index = torch.tensor([[0, 1], [1, 0], [1, 2], [2, 1]], dtype=torch.long)x = torch.tensor([[-1], [0], [1]], dtype=torch.float)data = Data(x=x, edge_index=edge_index.t().contiguous()) Data 对象包含了很多非常实用的函数： 12345678910111213141516171819202122232425262728293031323334353637print(data.keys)&gt;&gt;&gt; ['x', 'edge_index']print(data['x'])&gt;&gt;&gt; tensor([[-1.0], [0.0], [1.0]])for key, item in data: print(\"{} found in data\".format(key))&gt;&gt;&gt; x found in data&gt;&gt;&gt; edge_index found in data'edge_attr' in data&gt;&gt;&gt; Falsedata.num_nodes&gt;&gt;&gt; 3data.num_edges&gt;&gt;&gt; 4data.num_node_features&gt;&gt;&gt; 1data.contains_isolated_nodes()&gt;&gt;&gt; Falsedata.contains_self_loops()&gt;&gt;&gt; Falsedata.is_directed()&gt;&gt;&gt; False# Transfer data object to GPU.device = torch.device('cuda')data = data.to(device) 可以在 torch_geometric.data.Data 查看所有方法。 基准数据集PyG 包含了大量的基准数据集，使用这些数据集是非常简单的，初始化数据集的时候会自动下载对应的原始文件，并进行预处理成 Data 格式。 加载 ENZYMES 数据集，包含 600 个图，6 个类别： 1234567891011121314151617dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')&gt;&gt;&gt; ENZYMES(600)len(dataset)&gt;&gt;&gt; 600dataset.num_classes&gt;&gt;&gt; 6dataset.num_node_features&gt;&gt;&gt; 3data = dataset[0]&gt;&gt;&gt; Data(edge_index=[2, 168], x=[37, 3], y=[1])data.is_undirected()&gt;&gt;&gt; True 我们能看到，第一张图包含 37 个节点，每个节点有 3 个特征，有 168/2 = 84 个无向边，被分配给了一个类别。 小批量神经网络通常是以批量的方式训练的。PyG 通过创建稀疏块对角线邻接矩阵(由 edge_index 定义) ，并在节点维上串联特征矩阵和目标矩阵，实现了小批量的并行化。 数据变换图学习方法制作数据集PyG 已经为我们准备了很多基准数据集，但是当我们使用其他数据的时候还是需要自己构造。 PyG 有两个抽象的类，torch_geometric.data.Data 和 torch_geometric.data.InMemoryDataset，torch_geometric.data.InMemoryDataset 继承自 torch_geometric.data.Data，如果内存可以容纳数据集，推荐使用 InmemoryDataset。 跟从 torchvision 的约定，每个数据集都应该指明一个根文件夹用来指明数据存储的位置。根文件中包含两个子文件夹，raw_dir 存储原始数据，processed_dir 用来存储处理过后的数据。 创建“在内存中的数据集”需要实现四个基本的方法： torch_geometric.data.InMemoryDataset.raw_file_names(): raw_dir 文件中不需要被下载的文件列表。 torch_geometric.data.InMemoryDataset.processed_file_names(): processed_dir 文件中不需要被处理的文件列表 torch_geometric.data.InMemoryDataset.download(): 下载原始文件到 raw_dir。 torch_geometric.data.InMemoryDataset.process(): 处理原始文件，并把处理后的文件保存在 processed_dir。 你能从 torch_geometric.data 找到一些有用的函数去下载以及执行和提取数据。 process 是非常关键的函数，其中需要读取处理和保存数据。因为保存一个巨大的 python 列表特别慢，在保存之前我们通过 torch_geometric.data.InMemoryDataset.collate() 整理列表到一个巨大 Data 对象中。经过排序后的数据对象把所有的示例连接到一个大的数据对象之中，并返回一个字典，以便可以重构每个示例。最后我们将这两个对象加载到属性 self.data 和 self.slices。 创建“巨大的”数据集当数据不能完全加载到内存之中的时候，我们可以使用 torch_geometric.data.Data，这个类和 torchvision 的数据集非常的相近。它还需实现另外的两个函数： torch_geometric.data.Dataset.len(): 返回数据集中 examples 的数量。 torch_geometric.data.Dataset.get(): 实现加载单个图的逻辑。 在内部，torch_geometric.data.Dataset.__getitem__() 从 torch_geometric.data.Dataset.get() 获取数据对象，并通过 transform 对他们进行变换。 让我们通过一个简单的例子看下这个过程 123456789101112131415161718192021222324252627282930313233343536373839404142import os.path as ospimport torchfrom torch_geometric.data import Datasetclass MyOwnDataset(Dataset): def __init__(self, root, transform=None, pre_transform=None): super(MyOwnDataset, self).__init__(root, transform, pre_transform) @property def raw_file_names(self): return ['some_file_1', 'some_file_2', ...] @property def processed_file_names(self): return ['data_1.pt', 'data_2.pt', ...] def download(self): # Download to `self.raw_dir`. def process(self): i = 0 for raw_path in self.raw_paths: # Read data from `raw_path`. data = Data(...) if self.pre_filter is not None and not self.pre_filter(data): continue if self.pre_transform is not None: data = self.pre_transform(data) torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(i))) i += 1 def len(self): return len(self.processed_file_names) def get(self, idx): data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(idx))) return data 以上，每一个 graph 数据对象分别在 process() 中被保存，在 get() 函数中被加载。 常见问答： 我真的需要使用这些 dataset 接口吗？ 不，和其他普通的 pytorch 程序一样，你不需要使用 dataset，例如当你想创建临时的动态数据，而不是显示的保存在硬盘。在这种情况下，可以简单的传递一个包含 torch_geometric.data.Data 对象的 python list，然后传递他们到 torch_geometric.data.DataLoader。 1234from torch_geometric.data import Data, DataLoaderdata_list = [Data(...), ..., Data(...)]loader = DataLoader(data_list, batch_size=32) 进阶 mini-batchingmini-batching 对于深度学习训练大量的数据是至关重要的，可以进行并行训练，而不是一个一个的处理。在图像处理和自然语言处理领域，这是一种典型的做法，通过调节或者填充每一个 example 到同一个尺寸，然后对数据进行分组，分组长度就是 batch_size。 因为 graph 是最通用的数据结构，可以有任意的节点和边，以上的两种方法要么是不可行的，要么会导致大量非必需的内存消耗。在 PyG 中，我们选择了另一个方法去实现多个 examples 的并行。邻接矩阵是以对角方式堆叠，一个大图里面包含多个孤立的小图，节点和目标特征被检点的连接在一起。 $\\mathbf{A} = \\begin{bmatrix} \\mathbf{A}_1 &amp; &amp; \\ &amp; \\ddots &amp; \\ &amp; &amp; \\mathbf{A}_n \\end{bmatrix}, \\qquad \\mathbf{X} = \\begin{bmatrix} \\mathbf{X}_1 \\ \\vdots \\ \\mathbf{X}_n \\end{bmatrix}, \\qquad \\mathbf{Y} = \\begin{bmatrix} \\mathbf{Y}_1 \\ \\vdots \\ \\mathbf{Y}_n \\end{bmatrix}.$ 对比其他 batching 方式，这种具有一些重要的优势： GNN 操作依赖于信息传递，不需要被修改，因为属于两个不同 graph 的节点不会进行信息交换。 没有计算和内存开销。例如，这种方式没有进行对边或者节点有任何填充。需要指出的是，对于邻接矩阵来说没有任何额外的内存开销，因为他们是被以稀疏矩阵的方式进行保存，只保存非零数据。 PyG 在 torch_geometric.data.DataLoader 类的帮助下，可以自动的多个图形处理成一个巨大的图。在内部，torch_geometric.data.DataLoader 只是一个普通的 pytorch DataLoader，重写它的 collate() 功能，例如定义如何示例该被如何组织在一起。因此，所有可以被传递给 pytorch DataLoader 的参数，都可以被传递给 PyG 的 DataLoader，例如 num_workers。 通常情况下，PyG DataLoader 将要通过累积在全部图节点的数量自动的递增 edge_index tensor，这些图是在当前正在被处理的图之前被收集的。所有其他的张量都将在第一个维度中连接起来，而不会进一步增加它们的值。","link":"/PyG/"},{"title":"PostgreSQL 迁移数据","text":"记录 postgresql 数据库迁移中遇见的一个问题 web服务使用的腾讯云 serverless 服务，在重新部署的过程中提示： 1警告：你正在使用Serverless Component的beta测试版本。请使用正式的版本以获得更多的特性，详情请见：https://github.com/serverless/components/blob/master/README.cn.md 经过查询发现，自己使用的 component 还是 v1 版本，v2 版本已经发布，在升级 postgresql 的过程中需要手动迁移数据库。 数据库备份1pg_dump -U username -h hostname -p port databasename -f filename.sql 数据库导入1psql -U username -h hostname -d desintationdb -p port -f filename.sql 在导入数据库之后，发现 web 服务不可用且 python manage.py migrate 也报错： 12345# error 1psycopg2.errors.UndefinedTable: relation \"user_wxmodel\" does not exist...# error 2psycopg2.errors.InvalidSchemaName: no schema has been selected to create in... 解决在 pg_dump 备份文件中，注释掉下面这行即可： 1SELECT pg_catalog.set_config('search_path', '', false); 原因no schema has been selected to create in … error 参考 1","link":"/bug/PostgreSQL%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE/"},{"title":"那些年我写的现代诗","text":"记录和整理我写的现代诗 流年体警告！！！ 慎入！！！ 章一篇一 恨不能与你早相遇那样便可早相知 正好赶上三月的樱花风暖云疏 天气晴好 可终究我还是错过了那个大雪纷飞的季节我还在寻找机会与你接近却已经有人为你嘘寒问暖 篇二 崇法寺塔 高三，转移去了老校区，教室窗户外面就是两千多年前的古塔 我见证过繁华我经历过战火也许是这些原因我被永久的保护了起来这就是我的宿命像墓碑一样的伫立着 风吹日晒就换上新的琉璃岁月流逝就撑起铁的支架 可笑的世人啊你们不知自从那持灯的僧人走后我体内剩下的只是千百座没有温度的佛像 篇三 暗恋一个人，即便不能告白，也别藏得太深吧 我把所有的爱慕埋藏在难以触及的心底好让在遇见你的时候不漏声色 我把所有的思念放置在无人翻阅的诗集好让在没有你的日子笑靥如初 后来把自己也藏了起来藏在拥挤的人群藏入忙碌的日子 篇四想你不需要理由也不分季候 想你不是从别离开始在第一次遇见你之后失眠了一整个夜一整夜的风和雨风里雨里都是你 篇五 缘由是如何清澈的月光才能刺痛温柔的心是如何呜咽地笛声才能触动眼中的泪 今夜月 清如水此间笛 声如斯 可千万别别把这误以为我悲伤的缘由 章二 求打醒章节 篇一 快，来个人，打醒他 假如上天有意为你我安排一段姻缘我便不再担心不再着急只等你来爱我 然后执子之手到天荒地老 篇二 画师 接着打，往死里打 只希望我的世界是一片空白的画布任我去着色添笔 不需要昂贵的画布也不渴求绚丽的色彩只要一杆画笔我可以在篱墙下在田间阡陌在世界上的任何一个容得下你我的角落把你画在我的怀中把我画进你的心里 篇三 不要轻易承诺吧 如果你还不懂这些苦涩的诗句那就再等上些时日 等花开花落四季轮回 如果你还不懂我有多爱你那就再等上些时日 等海枯石烂誓言未改 章三篇一 天各一方 海南的气候让我难以忍受，希望你北方快乐 我南下你北上 南下有骄阳北方应有花香 篇二 a: 你喜欢我吗，b: 你能别问了吗 喜欢 不去问夏夜有多短不去问冬夜有多长春风还会如期拂面秋雨还会如约连绵 你也不必反复追问我会待你眉眼如初 篇三好多天没能想起你这南方九十度的阳光吞噬了所有的喜怒哀乐只剩下唯有的动作从一个眉梢到另一个眉梢不经意的重复就像以前不经意的想起你 我喜欢北方的冬天那可以让我想起温暖想起温暖就会想起你我喜欢那萧萧落木那可以让我感到孤独感到孤独也会想起你 我还喜欢北方的你 篇四夜无言只将天地笼罩 我是一颗北极星悄悄来到你的夜空怕惊醒你的美梦 你是否也会深夜凭栏斗转星移我还在那里 篇五 导演你说什么就是什么天晴了，又雨了在一起便在一起说别离就是别离 后来你让我哭我也比谁哭的都卖力 篇六 雨夹雪 到家的时候下了一场雨夹雪。希望两情相悦的人都能克服困难，终成眷属吧 你是我的前生我是你的来世 今夜天空飘落的是一场雨夹雪 章四篇一 诗人竟厌恶了这安逸的生活想要背起行囊做一个诗人 流浪 流浪去千里外的渔场从港口出发借北吹的风要两天航程父亲就在那里风吹雨打半辈子 我是来感受海的孤独风的刺骨一会就走 篇二 很小的时候，贪玩，每次周日，就开着钨丝灯，趴在椅子上赶作业 关于那段岁月的一切都值得珍藏 在那个年代里有灯，昏黄的要把傍晚的时间赶回来我趴在椅子上头也不抬 篇三 小学的时候，老师经常教育我们要珍惜时间，例子就是时钟走过就转不回去… 再也回不去了那年夏天月季盛开了一个季节三叶草换成了石板路蜜蜂不再飞来 再也回不去的呀是教室里的挂钟老师曾经反复强调 篇四 清明 高一的时候，清明节要出去玩，就在迈出家门的前一步，月考成绩出来了。。。高中再无清明节 亏欠故乡一个清明那年许好的承诺待到闲时一定要将清明的春色踏遍 那年我就从故乡飞了出去飞到了一个四季如夏的南方花草不是故乡的树木不是故乡的就连从北方吹来的风也变了味道 故乡的清明啊，再遇你时你若还是识得我请迎我以明媚春光请迎我以纷纷细雨 篇五 远行 16 年春节，一个人坐火车回海南，路上发生的事 醒来的第一句竟是家乡话半路上来的师傅没能听懂 原来我已离家千万里 章五篇一 诗三百 诗三百新解 你如星子划过美丽了我的回忆 别离之后我用三百诗篇洒满了夏夜演绎着你的昔日我的昨天 篇二 高二，春天，倒春寒，学校的花大面积阵亡 那两天春风渐暖暖得让人醉醉到花红柳也绿绿了千里河山山上有翠柳青枝枝头有莺歌燕舞 东风易变变料峭春寒寒风过处处处花残 篇三 同上篇 在春风刚来的日子里暖风吹得杨柳绿 百花开温柔的抚摸着荒凉了一个季节的田野村前刚化冻的河边野鸭正成群游过 可等时令一变都跟着变了模样风不再温柔光不再明媚 只是可怜胸膛这颗不分季候的心还那般炽热 篇四 同窗 大一，同窗换了学校复读，梦里在母校碰到了他 不应该遇见你的你我都已经离去 是你意外的出现还是我忽然地闯入 这次不是命运的安排是我一个小小的梦 篇五 这篇太矫情了。。。 不愿意醉去怕遗漏你温柔的话语醒时只剩满天风和雨 不愿意哭泣怕模糊我最后的记忆今日你是如此的美丽 亭外柳花年年枝无数不愿折柳相赠远行人怕柳色青青不敌关山万里失颜色 来生怎舍得与你相遇怕山盟海誓不堪世事变易伤别离 章六篇一 我很后悔去海南读书，有些遗憾此生都难以弥补 以前想看的海，看一次就够了以前想走的山，走一次就累了舅舅家里的无花果熟了没庭院里不知名的植物还没有开花 儿时多少的欢乐都交付给了昨日那个从外面捎来奇异的水果的人也走了没有人再给我带来那伴随我长大的玩具 知道到了天涯海角的距离曾经的那个男儿志在四方现在不想去远的地方闯荡 篇二你不知道有个人还在为你伤心你不知道有个人还在为你哭泣 岁月轮回 日夜更替千呼万唤也唤不回你 想让风吹尽思念想让雪掩埋回忆最想让时光倒流","link":"/article/poems/"},{"title":"The tutorial of Hexo and Icarus theme","text":"My Customzhangxu3486432/hexo-theme-icarus compare changes fix back-to-top bug add some icons add 163 music distinguish between thumbnail and cover modify profile layout add dropdown for toc … Quick StartInstall1npm install -g hexo SetupCreate Blog Project12345hexo init [blog_name]cd [blog_name]git initgit add .git commit -m 'init blog' Install Theme1git submodule add https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus Run1hexo s Deploy1234567891011# Remove generated files and cache.hexo clean# Generate static files.hexo generate# Deploy your website.hexo deploy# in shorthexo c &amp;&amp; hexo g &amp;&amp; hexo g Command12345678910111213141516Usage: hexo &lt;command&gt;Commands: clean Remove generated files and cache. config Get or set configurations. deploy Deploy your website. generate Generate static files. help Get help on a command. init Create a new Hexo folder. list List the information of the site migrate Migrate your site from other system to Hexo. new Create a new post. publish Moves a draft post from _drafts to _posts folder. render Render files with renderer plugins. server Start the server. version Display version information. ConfigGet or set configurations. 123Arguments: name Setting name. Leave it blank if you want to show all configurations. value New value of a setting. Leave it blank if you just want to show a single configuration. DeployDeploy your website. 123Options: --setup Setup without deployment -g, --generate Generate before deployment GenerateGenerate static files. 123456Options: -b, --bail Raise an error if any unhandled exception is thrown during generation -c, --concurrency Maximum number of files to be generated in parallel. Default is infinity -d, --deploy Deploy after generated -f, --force Force regenerate -w, --watch Watch file changes InitCreate a new Hexo folder at the specified path or the current directory. 123456Arguments: destination Folder path. Initialize in current folder if not specifiedOptions: --no-clone Copy files instead of cloning from GitHub --no-install Skip npm install ListList the information of the site. 12Arguments: type Available types: page, post, route, tag, category MigrateMigrate your site from other system to Hexo. 12Arguments: type Migrator type. NewCreate a new post. 12345678Arguments: layout Post layout. Use post, page, draft or whatever you want. title Post title. Wrap it with quotations to escape.Options: -p, --path Post path. Customize the path of the post. -r, --replace Replace the current post if existed. -s, --slug Post slug. Customize the URL of the post. PublishMoves a draft post from _drafts to _posts folder. 123Arguments: filename Draft filename. \"hello-world\" for example. layout Post layout. Use post, page, draft or whatever you want. RenderRender files with renderer plugins (e.g. Markdown) and save them at the specified path. 1234Options: --engine Specify render engine --output Output destination. Result will be printed in the terminal if the output destination is not set. --pretty Prettify JSON output ServerStart the server and watch for file changes. 123456Options: -i, --ip Override the default server IP. Bind to all IP address by default. -l, --log [format] Enable logger. Override log format. -o, --open Immediately open the server url in your default web browser. -p, --port Override the default port. -s, --static Only serve static files. Global Options1234567Global Options: --config Specify config file instead of using _config.yml --cwd Specify the CWD --debug Display all verbose messages in the terminal --draft Display draft posts --safe Disable all plugins and scripts --silent Hide output on console The directory structure12345678910111213├── _config.yml├── scaffolds│ ├── draft.md│ ├── page.md│ └── post.md├── source│ ├── _data│ ├── _drafts│ ├── _posts│ └── static└── themes ├── icarus └── landscape Configuration and Othershexo document icarus document Custom domaincreate a file named CNAME, fill blog.adream.win in it: 1echo blog.adream.win &gt; CNAME 1url: blog.adream.win github will generate certificates for your domain within 24 hours and then you can access blog.adream.win with https. IssueNeed to change the github OAuth Apps callback address after customizing the domain name, if you ever use it: gitalk cannot create issue SEOGoogleGoogle Search Console Baidu[Baidu]","link":"/software/The-tutorial-of-hexo-and-Icarus-theme/"},{"title":"城市异常事件检测","text":"","link":"/paper/%E5%9F%8E%E5%B8%82%E5%BC%82%E5%B8%B8%E4%BA%8B%E4%BB%B6%E6%A3%80%E6%B5%8B/"},{"title":"UCAS Affair Scripts","text":"项目地址 开发/收集 中国科学院大学 各种事务的自动化脚本 已有脚本： 选课 监听成绩 期待贡献，添加更多有趣的脚本，kill TodoList item Warning: 由于 选课系统 只能单点登录，如果同时运行两个脚本，或者在运行脚本的时候登陆网页端，他们之前会争抢会话。 脚本默认在会话失效后再次自动获取，所以在刷课和监听成绩的过程中，如果您需要浏览选课系统的网页端，需要先暂停脚本。 script12kill -STOP $pid # suspendkill -CONT $pid # resume UsageInitializationPull project: script1git clone https://github.com/zhangxu3486432/UCAS-Affair-Scripts.git Enter the working directory: script1cd UCAS-Affair-Scripts Install dependency: script1pip3 install -r deploy/requirements.txt Configuration Set in the settings.py Set http://sep.ucas.ac.cn/ UserName and PassWord 12USERNAME = ''PASSWORD = '' Login In the off-campus network login need to fill in the Verification Code. First login to save cookies in sep.cookie, so that you can facilitate deployment in the server. script1python3 login.py The Verification Code picture is saved in verification.png, open the picture identification key and fill it in terminal. As you can see, this Verification Code is 7351. Fill the 7351 in terminal and press enter. RUN take-courses Set the courses which you want to take Examples: 1COURSES = ['自然语言处理', '机器人智能控制', '积极心理学'] Fill in the college to which the course you choose belongs Examples: 1COLLEGES = ['人工智能学院', '心理学系'] Run script1python3 take_courses.py Run monitor_grades Set email info if you want to monitor your grads, you need to set it 1234SEND_EMAIL = ''SEND_EMAIL_PWD = ''RECEIVE_EMAIL = '' Run script1python3 monitor_grades.py TodoList Test and adapt to more python versions Shuttle Bus Reservation Build Docker Image Automatic Identification Verification Code","link":"/software/UCAS-Affair-Scripts/"},{"title":"software&#x2F;Untitled","text":"","link":"/software/Untitled/"},{"title":"elasticsearch 调优","text":"记录 elasticsearch 的使用及调优 max_map_count虚拟机最多可以使用的内存映射数量，每个映射空间的大小可能是 64KB、256KB等等，大小并不固定。 默认值为 65536，对于 elasticsearch 来说太少了，推荐增大到 262144 1sysctl -w vm.max_map_count=262144 上述方法在虚拟机重启后将失效，可以在 /etc/sysctl.conf 追加定义 1vm.max_map_count=262144 文件夹权限问题Error info 12java.lang.IllegalStateException: Failed to create node environmentjava.nio.file.AccessDeniedException: /usr/share/elasticsearch/data/nodes 改编权限 1sudo chown -R 1000:1000 elastic 内存禁用交换内存交换内存会导致性能严重下降 1bootstrap.memory_lock=true 也可以在操作系统级别禁用交换内存，或者降低 swappiness 的值，这个值决定操作系统交换内存的频率。 1vm.swappiness = 1 //swappiness 设置为 1 比设置为 0 要好，因为在一些内核版本 swappiness 设置为 0 会触发系统 OOM-killer。 JVM 内存空间elasticsearch 默认会分配 1GB 的内存空间，但是在生产环境中需要根据情况进行增加。 设置ES的内存堆有三种办法 通过 ES_HEAP_SIZE 来设置 min、max，此时两个值相等［推荐］ 通过 ES_MIN_MEM 和 ES_MAX_MEM 直接设置 使用 ES_JAVA_OPTS 来设置，ES_JAVA_OPTS=”-Xms=512M -Xmx=512M” 注意：如果值被同时设置时，优先级方法3 &gt; 方法1 &gt; 方法2 设置原则 Xmx 和 Xms 设置为相等的值，防止程序在运行时改变堆内存大小，这是一个很耗系统资源的过程。 不超过物理内存的 50%，elasticsearch 还需要用于 JVM heap 之外的其他目的； 不超过启用 compressed object pointers 的阀值，确切的阀值是不同的，但是接近 32GB，可以通过日志进行查看是否启用了 compressed object pointers：heap size [15.7gb], compressed ordinary object pointers [true]，你可以在你的 JVM 设置里添加 -XX:+PrintFlagsFinal 用来验证 JVM 的临界值； 最好不要超过启用 zero-based compressed oops 的阀值，确切的阀值是不同的，但是对于大部分的系统来说 26GB 是一个安全的值；可以在启动添加 JVM options -XX:+UnlockDiagnosticVMOptions -XX:+PrintCompressedOopsMode 显示是否启动了 zero-based compressed oops，启用：zero based Compressed Oops，未启用：Compressed Oops with base 假设你有个机器有 128 GB 的内存，你可以创建两个节点，每个节点内存分配不超过 32 GB。 也就是说不超过 64 GB 内存给 ES 的堆内存，剩下的超过 64 GB 的内存给 Lucene。 如果你选择这一种，你需要配置 cluster.routing.allocation.same_shard.host: true 。 这会防止同一个分片（shard）的主副本存在同一个物理机上（因为如果存在一个机器上，副本的高可用性就没有了）。 提高写入速度减少刷新频率减少刷新频率，降低潜在的写磁盘性能损耗，提高写入速度。 1234PUT /_all/_settings?preserve_existing=true{ \"index.refresh_interval\" : \"600s\"} replica 数目设置在 bulk 大量数据到ES集群的可以把副本数设置为0，在数据导入完成之后再设置为1或者你集群的适合的数目。 1234PUT /_all/_settings?pretty{ \"index.number_of_replicas\" : 0} 部署 调优 调优 写入性能优化","link":"/software/elasticsearch/"},{"title":"Git","text":"记录常用的 git 操作 更换 remote 仓库12345678# 删除远程仓库 origin 是远程仓库的名字git remote remove origin# 配置远程仓库git remote add origin git@github.com:zhangxu3486432/hexo-theme-icarus.git# 建立与远程上游的跟踪并推送git push --set-upstream origin master git pull 和 git fetch 的区别git pull = git fetch + git merge/rebase 参考 1参考 2","link":"/software/git/"},{"title":"postgresql 备份与恢复","text":"目前数据库有 7 百多万条数据，要是哪天数据库或者服务器炸了我也就凉透了，还是赶紧备份下吧 由于数据量非常大，使用 pg_dump 和 psql 进行导入导出是不现实的，需要使用增量备份。 关于 postgresql 的备份工具，大家可以去 awesome-postgres 这个项目查看，我选用的是 wal-g 这个工具，它可以无缝对接亚马逊 S3、Google Cloud 以及 MinIO 等对象存储服务，这也是我选用这个工具的原因，等以后有钱了就无缝上云。。。目前是使用的自建的 minio 对象存储服务。 安装 minio我是使用的 docker-compose，其他的安装方式请参考 官方文档 123456789101112131415161718192021222324version: '3'services: minio: image: minio/minio:RELEASE.2020-07-27T18-37-02Z container_name: minio volumes: - minio-data:/minio-data ports: - \"9000:9000\" environment: MINIO_ACCESS_KEY: youkey MINIO_SECRET_KEY: yourpassword command: server /minio-data healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"] interval: 30s timeout: 20s retries: 3 volumes: minio-data: 创建 bucket创建 minio 中用于存放数据库备份存储桶。 使用 mc(minio client) 创建 1mc mb local/pgsqlbkp # local 根据实际情况替换成 minio 的别名 登录 minio web 管理端，通过图形界面创建 安装 wal-g可以直接在 Releases tab 下载二进制文件。 12tar -zxvf wal-g.linux-amd64.tar.gzmv wal-g /usr/local/bin/ 安装 postgresql为了安全起见，需要将数据挂载在宿主机上，进行持久化 1- ./data/postgresql/data:/var/lib/postgresql/data 在容器内，需要使用 wal-g 备份 postgresql 产生的 wal 文件，所以也需要将之前在宿主机安装的 wal-g 程序挂载到容器内 1- /usr/local/bin/wal-g:/usr/local/bin/wal-g 配置 postgresql wal_level archive 或更高级别 archive_mode on 开启归档模式 archive_command 归档时触发的命令或脚本 archive_timeout 可以理解为超过指定时间强制执行 select pg_switch_wal(); 场景，数据库不是很活跃，数据库wal日志产生的过慢时。如果这个值没有单位，默认将会是秒。 restore_command 恢复时触发的命令或脚本 recovery_target_time 需要恢复到的时间点 归档触发条件说明： 手动执行 select pg_switch_wal()； WAL 日志写满后触发归档 WAL 日志文件，默认为 16MB； 如果设置 archive_timeout，超时触发。 完成的配置 12345678910111213141516171819202122232425version: '3'services: postgres: image: postgres container_name: postgres restart: always environment: - POSTGRES_DB=postgres - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres - TZ=Asia/Shanghai - WALG_S3_PREFIX=s3://pgsqlbkp/ - AWS_ACCESS_KEY_ID=youak - AWS_SECRET_ACCESS_KEY=yourpassword - AWS_ENDPOINT=http://ip:9000 - AWS_S3_FORCE_PATH_STYLE=true command: [\"postgres\", \"-c\", \"wal_level=replica\", \"-c\", \"archive_mode=on\", \"-c\", \"archive_command=wal-g wal-push %p\", \"-c\", \"restore_command=wal-g wal-fetch %f %p\", \"-c\", \"recovery_target_time=2021-01-01 00:00:00\"] volumes: - ./data/postgresql/data:/var/lib/postgresql/data - /usr/local/bin/wal-g:/usr/local/bin/wal-g ports: - 5432:5432 备份经过上面的设置，在 postgresql 启动以后，就开始不断地向 minio 服务器备份 wal 归档文件，另外我们还需要备份数据库，恢复的逻辑是这样的： 1时间点 A 的数据 = 恢复（时间点 B 的数据库，时间点 B 到 A 之间的 wal 文件） 首先设置环境变量，在 postgresql 容器中已经被设置过了，被用来备份 wal 文件，在宿主机上还需要重新设置一遍，用来备份数据库 1234567891011# wal-g.env# source wal-g.envexport PGDATA=/path/postgresql/data # postgresql 数据存放的路径export WALG_S3_PREFIX=\"s3://pgsqlbkp/\"export PGPORT=5432export PGUSER=postgresexport PGPASSWORD=postgresexport AWS_ACCESS_KEY_ID=youakexport AWS_SECRET_ACCESS_KEY=yourpasswordexport AWS_ENDPOINT=\"http://ip:9000\"export AWS_S3_FORCE_PATH_STYLE=\"true\" 备份 1wal-g backup-push $PGDATA 因为日志在一定的体积之后才会触发归档日志，备份完之后需要做一次 WAL 切换，保证最新的WAL日志归档到归档目录 1select pg_switch_wal(); 恢复将之前的数据库清空，模拟数据库损坏 之前在网上看过很多教程，需要在 postgresql 地址中创建 recovery.conf 123restore_command = 'wal-g wal-fetch %f %p'standby_mode = onrecovery_target_time = '2222-11-11 00:00:00' 但是在 postgresql 12 之后，这个文件就被取消了，参见，如果在 postgresql 出现 recovery.conf 文件，pg 就会报错 1FATAL: using recovery command file \"recovery.conf\" is not supported 取而代之的是需要创建在原来的位置创建两个标识文件，来表示恢复数据库和 standby_mode standby_mode为off时，应用完所有日志后，自动退出恢复，进入运行状态。standby_mode为on时，应用完所有日志后，恢复流程不会退出，持续读取可用日志（来自于归档日志文件或流复制），当收到pg_ctl工具发出的promote命令后，才退出恢复流程，进入运行状态。 recovery.signal standby.signal restore_command 和 recovery_target_time 被合并在了 postgresql.conf，在之前的 postgresql yaml 里面已经被定义 ↑ 查看 minio 中的备份列表 1wal-g backup-list --pretty 123456+---+-------------------------------+-----------------------------------+--------------------------+| # | NAME | LAST MODIFIED | WAL SEGMENT BACKUP START |+---+-------------------------------+-----------------------------------+--------------------------+| 0 | base_000000010000000000000006 | Wednesday, 29-Jul-20 16:23:22 UTC | 000000010000000000000006 || 1 | base_000000010000000000000004 | Wednesday, 29-Jul-20 16:22:49 UTC | 000000010000000000000004 |+---+-------------------------------+-----------------------------------+--------------------------+ 从远程下面备份到 postgresql 文件夹 123456# wal-g backup-fetch destination_directory backup_namewal-g backup-fetch $PGDATA base_000000010000000000000006# 或者wal-g backup-fetch $PGDATA LATEST 在 postgresql 文件夹文件夹下创建 recovery.signal（如果使用standby mode，需要创建 standby.signal） 12touch $PGDATA/recovery.signal# touch $PGDATA/standby.signal 然后等待恢复即可 1232020-07-30 23:41:18.011 CST [25] LOG: restored log file \"00000003000000000000000D\" from archive2020-07-30 23:41:18.152 CST [25] LOG: redo starts at 0/D0000602020-07-30 23:41:18.790 CST [1] LOG: database system is ready to accept connections 参考 参考 参考 参考","link":"/software/postgresql-backup/"},{"title":"Docker 设置时区","text":"简介Docker 默认时区是协调世界时，又称世界标准时间，简称 UTC 中国大陆、香港、澳门、台湾、蒙古国、新加坡、马来西亚、菲律宾、澳洲西部的时间与 UTC 的时差均为 +8，也就是 UTC+8。 中国标准时间又称为 CST(China Standard Time) 查看时区可以使用 date 命令查看时区信息 1234567➜ date# UTC 时间Sat Feb 21 16:15:16 UTC 2020➜ date# CST 时间Sat Feb 22 00:15:16 CST 2020 更改时区根据应用方式的不同修改 Docker 时区可分为三种方式 docker-compose12environment: TZ: Asia/Shanghai Dockerfile1234567891011# 需要安装 tzdata 依赖RUN apk add --no-cache tzdataENV TZ Asia/Shanghai# or# 若容器内不存在 /usr/share/zoneinfo，则需要创建这个目录RUN apk add --no-cache tzdata \\ &amp;&amp; ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ &amp;&amp; echo \"Asia/Shanghai\" &gt; /etc/timezoneENV TZ Asia/Shanghai docker run12# 容器的时区和主机的时区相同docker run -v /etc/timezone:/etc/timezone -v /etc/localtime:/etc/localtime -it [image name] sh","link":"/software/docker-timezone/"}],"tags":[{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"PostgreSQL","slug":"PostgreSQL","link":"/tags/PostgreSQL/"},{"name":"Bug","slug":"Bug","link":"/tags/Bug/"},{"name":"现代诗","slug":"现代诗","link":"/tags/%E7%8E%B0%E4%BB%A3%E8%AF%97/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"论文","slug":"论文","link":"/tags/%E8%AE%BA%E6%96%87/"},{"name":"异常检测","slug":"异常检测","link":"/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"},{"name":"综述","slug":"综述","link":"/tags/%E7%BB%BC%E8%BF%B0/"},{"name":"城市计算","slug":"城市计算","link":"/tags/%E5%9F%8E%E5%B8%82%E8%AE%A1%E7%AE%97/"},{"name":"中国科学院大学","slug":"中国科学院大学","link":"/tags/%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"备份","slug":"备份","link":"/tags/%E5%A4%87%E4%BB%BD/"},{"name":"对象存储","slug":"对象存储","link":"/tags/%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"}],"categories":[{"name":"论文","slug":"论文","link":"/categories/%E8%AE%BA%E6%96%87/"},{"name":"软件","slug":"软件","link":"/categories/%E8%BD%AF%E4%BB%B6/"},{"name":"杂记","slug":"杂记","link":"/categories/%E6%9D%82%E8%AE%B0/"},{"name":"城市计算","slug":"论文/城市计算","link":"/categories/%E8%AE%BA%E6%96%87/%E5%9F%8E%E5%B8%82%E8%AE%A1%E7%AE%97/"},{"name":"PostgreSQL","slug":"软件/PostgreSQL","link":"/categories/%E8%BD%AF%E4%BB%B6/PostgreSQL/"},{"name":"开源项目","slug":"开源项目","link":"/categories/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/"},{"name":"elasticsearch","slug":"软件/elasticsearch","link":"/categories/%E8%BD%AF%E4%BB%B6/elasticsearch/"},{"name":"git","slug":"软件/git","link":"/categories/%E8%BD%AF%E4%BB%B6/git/"},{"name":"Docker","slug":"软件/Docker","link":"/categories/%E8%BD%AF%E4%BB%B6/Docker/"}]}